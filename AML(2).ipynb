{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqfZA9BqAaUoaqr8IiliWv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kahram-y/AML_project/blob/main/AML(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "금융결제원 AML 탐지 프로젝트\n",
        "\n",
        "IBM Transactions for Anti Money Laundering 데이터셋 활용\n",
        "\n",
        "목표: 자금세탁 계좌 탐지 (노드 분류)\n",
        "\n",
        "프로젝트 구조:\n",
        "1. 데이터 탐색 및 전처리\n",
        "2. 문제 정의\n",
        "3. 피쳐 생성 (집계 피쳐 + 그래프 피쳐)\n",
        "4. 모델 학습 및 평가\n",
        "   - Baseline: XGBoost/CatBoost\n",
        "   - 시계열 모델 앙상블\n",
        "   - 그래프 피쳐 추가\n",
        "   - GNN 모델"
      ],
      "metadata": {
        "id": "0gKAeRnZ53IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn xgboost catboost lightgbm networkx matplotlib seaborn imbalanced-learn shap"
      ],
      "metadata": {
        "id": "Bk5hsQOH56u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "oDrm0VUg5-n2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 파일 경로\n",
        "trans_path = '/content/drive/MyDrive/HI-Medium_Trans.csv'\n",
        "accounts_path = '/content/drive/MyDrive/HI-Medium_accounts.csv'\n",
        "\n",
        "import os\n",
        "print(os.path.exists(trans_path))\n",
        "print(os.path.exists(accounts_path))"
      ],
      "metadata": {
        "id": "BdStiMoZ5_xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        },
        "id": "zlPo1cZp3FuO",
        "outputId": "ee4d2bc7-8904-468c-9d70-c891c9070d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Mounted at /content/drive\n",
            "True\n",
            "True\n",
            "================================================================================\n",
            "금융결제원 AML 탐지 프로젝트 시작\n",
            "================================================================================\n",
            "================================================================================\n",
            "데이터 로딩 중...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'HI-Medium_Trans.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-142774554.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;31m# 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-142774554.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    641\u001b[0m     )\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0mdf_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_accounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m     \u001b[0mdf_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_trans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_accounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-142774554.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# 거래 데이터 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# 계좌 데이터 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HI-Medium_Trans.csv'"
          ]
        }
      ],
      "source": [
        "# ====================================\n",
        "# 1. 데이터 로드 및 탐색\n",
        "# ====================================\n",
        "\n",
        "class AMLDataLoader:\n",
        "    \"\"\"AML 데이터 로드 및 초기 탐색\"\"\"\n",
        "\n",
        "    def __init__(self, trans_path, accounts_path):\n",
        "        self.trans_path = trans_path\n",
        "        self.accounts_path = accounts_path\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"데이터 로드\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"데이터 로딩 중...\")\n",
        "\n",
        "        # 거래 데이터 로드\n",
        "        self.transactions = pd.read_csv(self.trans_path)\n",
        "\n",
        "        # 계좌 데이터 로드\n",
        "        self.accounts = pd.read_csv(self.accounts_path)\n",
        "\n",
        "        print(f\"거래 데이터 shape: {self.transactions.shape}\")\n",
        "        print(f\"계좌 데이터 shape: {self.accounts.shape}\")\n",
        "\n",
        "        return self.transactions, self.accounts\n",
        "\n",
        "    def explore_data(self, df_trans, df_accounts):\n",
        "        \"\"\"데이터 탐색\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"데이터 탐색\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 거래 데이터 기본 정보\n",
        "        print(\"\\n[거래 데이터 샘플]\")\n",
        "        print(df_trans.head())\n",
        "        print(f\"\\n컬럼: {df_trans.columns.tolist()}\")\n",
        "        print(f\"\\n결측치:\\n{df_trans.isnull().sum()}\")\n",
        "\n",
        "        # Is Laundering 분포\n",
        "        if 'Is Laundering' in df_trans.columns:\n",
        "            laundering_dist = df_trans['Is Laundering'].value_counts()\n",
        "            print(f\"\\n[자금세탁 분포]\")\n",
        "            print(laundering_dist)\n",
        "            print(f\"자금세탁 비율: {laundering_dist[1] / len(df_trans) * 100:.4f}%\")\n",
        "\n",
        "        # 시간 정보 파싱 및 분포 확인\n",
        "        df_trans['Timestamp'] = pd.to_datetime(\n",
        "            df_trans['Timestamp'],\n",
        "            format='%Y/%m/%d %H:%M'\n",
        "        )\n",
        "        df_trans['Year'] = df_trans['Timestamp'].dt.year\n",
        "        df_trans['Month'] = df_trans['Timestamp'].dt.month\n",
        "        df_trans['Day'] = df_trans['Timestamp'].dt.day\n",
        "        df_trans['Hour'] = df_trans['Timestamp'].dt.hour\n",
        "        df_trans['DayOfWeek'] = df_trans['Timestamp'].dt.dayofweek\n",
        "\n",
        "        print(f\"\\n[시간 범위]\")\n",
        "        print(f\"시작: {df_trans['Timestamp'].min()}\")\n",
        "        print(f\"종료: {df_trans['Timestamp'].max()}\")\n",
        "\n",
        "        # Day별 자금세탁 분포 확인\n",
        "        if 'Is Laundering' in df_trans.columns:\n",
        "            day_dist = df_trans.groupby('Day')['Is Laundering'].agg(['sum', 'count', 'mean'])\n",
        "            print(f\"\\n[일별 자금세탁 분포]\")\n",
        "            print(day_dist)\n",
        "\n",
        "            # 치우침 확인\n",
        "            if day_dist['mean'].std() > 0.1:\n",
        "                print(\"\\n⚠️ Day 피쳐의 label 분포가 치우쳐져 있습니다. 제거 고려 필요\")\n",
        "\n",
        "        # 계좌 데이터 정보\n",
        "        print(f\"\\n[계좌 데이터 샘플]\")\n",
        "        print(df_accounts.head())\n",
        "        print(f\"\\n국가 분포:\\n{df_accounts['Bank'].value_counts().head(10)}\")\n",
        "\n",
        "        return df_trans\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 2. 데이터 전처리 및 샘플링\n",
        "# ====================================\n",
        "\n",
        "class AMLPreprocessor:\n",
        "    \"\"\"데이터 전처리 및 샘플링\"\"\"\n",
        "\n",
        "    def __init__(self, sample_ratio=0.1, random_state=42):\n",
        "        self.sample_ratio = sample_ratio\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def create_hourly_samples(self, df_trans):\n",
        "        \"\"\"시간 단위 배치로 모델 단위 생성\n",
        "        각 계좌번호 + 시간 단위로 샘플 생성\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"시간 단위 배치 샘플 생성\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # From Account 기준 샘플 생성\n",
        "        from_samples = df_trans.copy()\n",
        "        from_samples['Account'] = from_samples['From Bank'] + '_' + from_samples['Account'].astype(str)\n",
        "        from_samples['TimeUnit'] = from_samples['Timestamp'].dt.strftime('%Y-%m-%d %H')\n",
        "        from_samples['Direction'] = 'OUT'\n",
        "\n",
        "        # To Account 기준 샘플 생성\n",
        "        to_samples = df_trans.copy()\n",
        "        to_samples['Account'] = to_samples['To Bank'] + '_' + to_samples['Account.1'].astype(str)\n",
        "        to_samples['TimeUnit'] = to_samples['Timestamp'].dt.strftime('%Y-%m-%d %H')\n",
        "        to_samples['Direction'] = 'IN'\n",
        "\n",
        "        # 합치기\n",
        "        all_samples = pd.concat([from_samples, to_samples], ignore_index=True)\n",
        "\n",
        "        # 계좌 + 시간 단위로 그룹화하여 label 결정\n",
        "        # Is Laundering=1인 거래가 하나라도 있으면 해당 시간의 계좌는 suspicious\n",
        "        account_time_labels = all_samples.groupby(['Account', 'TimeUnit']).agg({\n",
        "            'Is Laundering': 'max',  # 하나라도 1이면 1\n",
        "            'Timestamp': 'min'\n",
        "        }).reset_index()\n",
        "\n",
        "        account_time_labels.rename(columns={'Timestamp': 'TimeUnit_Start'}, inplace=True)\n",
        "\n",
        "        print(f\"총 샘플 수 (계좌-시간 단위): {len(account_time_labels)}\")\n",
        "        print(f\"자금세탁 샘플: {account_time_labels['Is Laundering'].sum()}\")\n",
        "\n",
        "        return all_samples, account_time_labels\n",
        "\n",
        "    def stratified_sample(self, df, target_col='Is Laundering'):\n",
        "        \"\"\"계층화 샘플링\"\"\"\n",
        "        print(f\"\\n계층화 샘플링 (비율: {self.sample_ratio})\")\n",
        "\n",
        "        # 클래스별로 샘플링\n",
        "        sampled_dfs = []\n",
        "        for label in df[target_col].unique():\n",
        "            label_df = df[df[target_col] == label]\n",
        "            n_samples = int(len(label_df) * self.sample_ratio)\n",
        "            sampled = label_df.sample(n=n_samples, random_state=self.random_state)\n",
        "            sampled_dfs.append(sampled)\n",
        "\n",
        "        result = pd.concat(sampled_dfs, ignore_index=True)\n",
        "        print(f\"샘플링 후 크기: {len(result)}\")\n",
        "        print(f\"자금세탁 비율: {result[target_col].sum() / len(result) * 100:.4f}%\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def analyze_laundering_patterns(self, df_trans):\n",
        "        \"\"\"자금세탁 건과 정상 건의 차이 분석\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"자금세탁 패턴 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        laundering = df_trans[df_trans['Is Laundering'] == 1]\n",
        "        normal = df_trans[df_trans['Is Laundering'] == 0]\n",
        "\n",
        "        # 거래 금액 비교\n",
        "        print(f\"\\n[거래 금액 통계]\")\n",
        "        print(f\"자금세탁 - 평균: ${laundering['Amount Received'].mean():.2f}, \"\n",
        "              f\"중앙값: ${laundering['Amount Received'].median():.2f}\")\n",
        "        print(f\"정상 거래 - 평균: ${normal['Amount Received'].mean():.2f}, \"\n",
        "              f\"중앙값: ${normal['Amount Received'].median():.2f}\")\n",
        "\n",
        "        # 결제 수단 분포\n",
        "        print(f\"\\n[결제 수단 분포]\")\n",
        "        print(\"자금세탁:\")\n",
        "        print(laundering['Receiving Currency'].value_counts().head())\n",
        "        print(\"\\n정상 거래:\")\n",
        "        print(normal['Receiving Currency'].value_counts().head())\n",
        "\n",
        "        # 시간대 분포\n",
        "        print(f\"\\n[시간대 분포]\")\n",
        "        print(\"자금세탁 - 시간대별:\")\n",
        "        print(laundering['Hour'].value_counts().sort_index())\n",
        "\n",
        "        return laundering, normal\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 3. 피쳐 생성\n",
        "# ====================================\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"집계 피쳐 및 그래프 피쳐 생성\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "\n",
        "    def create_aggregation_features(self, df_trans, account_time_df):\n",
        "        \"\"\"50개 이상의 집계 피쳐 생성\n",
        "        과거 정보만 사용 (Data Leakage 방지)\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"집계 피쳐 생성\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        features_list = []\n",
        "\n",
        "        for idx, row in account_time_df.iterrows():\n",
        "            if idx % 10000 == 0:\n",
        "                print(f\"진행: {idx}/{len(account_time_df)}\")\n",
        "\n",
        "            account = row['Account']\n",
        "            time_unit = pd.to_datetime(row['TimeUnit_Start'])\n",
        "\n",
        "            # 해당 계좌의 과거 거래만 필터링\n",
        "            past_trans = df_trans[\n",
        "                (df_trans['Account'] == account) &\n",
        "                (df_trans['Timestamp'] < time_unit)\n",
        "            ].copy()\n",
        "\n",
        "            if len(past_trans) == 0:\n",
        "                # 과거 거래가 없으면 기본값\n",
        "                features = self._default_features(account, time_unit)\n",
        "            else:\n",
        "                features = self._compute_features(past_trans, account, time_unit)\n",
        "\n",
        "            features_list.append(features)\n",
        "\n",
        "            if idx >= 1000:  # 시연용으로 1000건만\n",
        "                break\n",
        "\n",
        "        feature_df = pd.DataFrame(features_list)\n",
        "        self.feature_names = [c for c in feature_df.columns\n",
        "                             if c not in ['Account', 'TimeUnit']]\n",
        "\n",
        "        print(f\"\\n생성된 피쳐 수: {len(self.feature_names)}\")\n",
        "        print(f\"피쳐 목록 (처음 10개): {self.feature_names[:10]}\")\n",
        "\n",
        "        return feature_df\n",
        "\n",
        "    def _compute_features(self, past_trans, account, time_unit):\n",
        "        \"\"\"실제 피쳐 계산\"\"\"\n",
        "        features = {'Account': account, 'TimeUnit': str(time_unit)}\n",
        "\n",
        "        # 시간 윈도우 정의\n",
        "        windows = {\n",
        "            '1h': timedelta(hours=1),\n",
        "            '3h': timedelta(hours=3),\n",
        "            '1d': timedelta(days=1),\n",
        "            '7d': timedelta(days=7)\n",
        "        }\n",
        "\n",
        "        for window_name, window_delta in windows.items():\n",
        "            window_start = time_unit - window_delta\n",
        "            window_trans = past_trans[past_trans['Timestamp'] >= window_start]\n",
        "\n",
        "            # OUT 거래 (송금)\n",
        "            out_trans = window_trans[window_trans['Direction'] == 'OUT']\n",
        "            features[f'out_count_{window_name}'] = len(out_trans)\n",
        "            features[f'out_amount_sum_{window_name}'] = out_trans['Amount Paid'].sum()\n",
        "            features[f'out_amount_mean_{window_name}'] = out_trans['Amount Paid'].mean() if len(out_trans) > 0 else 0\n",
        "            features[f'out_amount_std_{window_name}'] = out_trans['Amount Paid'].std() if len(out_trans) > 0 else 0\n",
        "            features[f'out_amount_max_{window_name}'] = out_trans['Amount Paid'].max() if len(out_trans) > 0 else 0\n",
        "\n",
        "            # IN 거래 (입금)\n",
        "            in_trans = window_trans[window_trans['Direction'] == 'IN']\n",
        "            features[f'in_count_{window_name}'] = len(in_trans)\n",
        "            features[f'in_amount_sum_{window_name}'] = in_trans['Amount Received'].sum()\n",
        "            features[f'in_amount_mean_{window_name}'] = in_trans['Amount Received'].mean() if len(in_trans) > 0 else 0\n",
        "            features[f'in_amount_std_{window_name}'] = in_trans['Amount Received'].std() if len(in_trans) > 0 else 0\n",
        "            features[f'in_amount_max_{window_name}'] = in_trans['Amount Received'].max() if len(in_trans) > 0 else 0\n",
        "\n",
        "            # 순 흐름\n",
        "            features[f'net_flow_{window_name}'] = (\n",
        "                features[f'in_amount_sum_{window_name}'] -\n",
        "                features[f'out_amount_sum_{window_name}']\n",
        "            )\n",
        "\n",
        "            # 외화 거래\n",
        "            foreign_curr = window_trans[\n",
        "                window_trans['Payment Currency'] != window_trans['Receiving Currency']\n",
        "            ]\n",
        "            features[f'foreign_count_{window_name}'] = len(foreign_curr)\n",
        "            features[f'foreign_ratio_{window_name}'] = (\n",
        "                len(foreign_curr) / len(window_trans) if len(window_trans) > 0 else 0\n",
        "            )\n",
        "\n",
        "        # 전체 거래 통계\n",
        "        features['total_trans_count'] = len(past_trans)\n",
        "        features['total_out_amount'] = past_trans[past_trans['Direction'] == 'OUT']['Amount Paid'].sum()\n",
        "        features['total_in_amount'] = past_trans[past_trans['Direction'] == 'IN']['Amount Received'].sum()\n",
        "\n",
        "        # 거래 상대방 다양성\n",
        "        features['unique_counterparties'] = past_trans['Account'].nunique()\n",
        "\n",
        "        # 결제 수단 다양성\n",
        "        features['unique_currencies'] = past_trans['Payment Currency'].nunique()\n",
        "\n",
        "        # 시간대 분포\n",
        "        hour_dist = past_trans['Hour'].value_counts()\n",
        "        features['night_trans_ratio'] = (\n",
        "            hour_dist[(hour_dist.index >= 0) & (hour_dist.index < 6)].sum() /\n",
        "            len(past_trans) if len(past_trans) > 0 else 0\n",
        "        )\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _default_features(self, account, time_unit):\n",
        "        \"\"\"과거 거래가 없을 때 기본 피쳐\"\"\"\n",
        "        features = {'Account': account, 'TimeUnit': str(time_unit)}\n",
        "\n",
        "        # 모든 피쳐를 0으로 초기화\n",
        "        windows = ['1h', '3h', '1d', '7d']\n",
        "        for window in windows:\n",
        "            for prefix in ['out', 'in']:\n",
        "                for metric in ['count', 'amount_sum', 'amount_mean', 'amount_std', 'amount_max']:\n",
        "                    features[f'{prefix}_{metric}_{window}'] = 0\n",
        "            features[f'net_flow_{window}'] = 0\n",
        "            features[f'foreign_count_{window}'] = 0\n",
        "            features[f'foreign_ratio_{window}'] = 0\n",
        "\n",
        "        features['total_trans_count'] = 0\n",
        "        features['total_out_amount'] = 0\n",
        "        features['total_in_amount'] = 0\n",
        "        features['unique_counterparties'] = 0\n",
        "        features['unique_currencies'] = 0\n",
        "        features['night_trans_ratio'] = 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def create_graph_features(self, df_trans, account_time_df):\n",
        "        \"\"\"그래프 기반 피쳐 생성\n",
        "        - Centrality 기반 (Degree, Closeness, Betweenness)\n",
        "        - Path & Flow 패턴\n",
        "        - Community 구조\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"그래프 피쳐 생성\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 거래 네트워크 구축\n",
        "        G = nx.DiGraph()\n",
        "\n",
        "        for _, row in df_trans.iterrows():\n",
        "            from_acc = row['From Bank'] + '_' + str(row['Account'])\n",
        "            to_acc = row['To Bank'] + '_' + str(row['Account.1'])\n",
        "            amount = row['Amount Paid']\n",
        "\n",
        "            if G.has_edge(from_acc, to_acc):\n",
        "                G[from_acc][to_acc]['weight'] += amount\n",
        "                G[from_acc][to_acc]['count'] += 1\n",
        "            else:\n",
        "                G.add_edge(from_acc, to_acc, weight=amount, count=1)\n",
        "\n",
        "        print(f\"그래프 구축 완료 - 노드: {G.number_of_nodes()}, 엣지: {G.number_of_edges()}\")\n",
        "\n",
        "        # Centrality 계산\n",
        "        print(\"Centrality 계산 중...\")\n",
        "        degree_centrality = nx.degree_centrality(G)\n",
        "        in_degree_centrality = nx.in_degree_centrality(G)\n",
        "        out_degree_centrality = nx.out_degree_centrality(G)\n",
        "\n",
        "        # Betweenness는 계산량이 많으므로 샘플링\n",
        "        sample_nodes = list(G.nodes())[:min(1000, len(G.nodes()))]\n",
        "        betweenness_centrality = nx.betweenness_centrality(\n",
        "            G.subgraph(sample_nodes),\n",
        "            weight='weight'\n",
        "        )\n",
        "\n",
        "        # PageRank\n",
        "        pagerank = nx.pagerank(G, weight='weight')\n",
        "\n",
        "        # 그래프 피쳐를 데이터프레임에 추가\n",
        "        graph_features = []\n",
        "\n",
        "        for _, row in account_time_df.iterrows():\n",
        "            account = row['Account']\n",
        "\n",
        "            features = {\n",
        "                'Account': account,\n",
        "                'TimeUnit': row['TimeUnit'],\n",
        "                'degree_centrality': degree_centrality.get(account, 0),\n",
        "                'in_degree_centrality': in_degree_centrality.get(account, 0),\n",
        "                'out_degree_centrality': out_degree_centrality.get(account, 0),\n",
        "                'betweenness_centrality': betweenness_centrality.get(account, 0),\n",
        "                'pagerank': pagerank.get(account, 0),\n",
        "            }\n",
        "\n",
        "            # 이웃 노드 정보\n",
        "            if account in G:\n",
        "                successors = list(G.successors(account))\n",
        "                predecessors = list(G.predecessors(account))\n",
        "\n",
        "                features['num_successors'] = len(successors)\n",
        "                features['num_predecessors'] = len(predecessors)\n",
        "                features['total_out_weight'] = sum(G[account][s]['weight'] for s in successors)\n",
        "                features['total_in_weight'] = sum(G[p][account]['weight'] for p in predecessors)\n",
        "            else:\n",
        "                features['num_successors'] = 0\n",
        "                features['num_predecessors'] = 0\n",
        "                features['total_out_weight'] = 0\n",
        "                features['total_in_weight'] = 0\n",
        "\n",
        "            graph_features.append(features)\n",
        "\n",
        "        graph_feature_df = pd.DataFrame(graph_features)\n",
        "\n",
        "        print(f\"그래프 피쳐 생성 완료 - 피쳐 수: {len(graph_feature_df.columns) - 2}\")\n",
        "\n",
        "        return graph_feature_df\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 4. 모델 학습 및 평가\n",
        "# ====================================\n",
        "\n",
        "class AMLModelTrainer:\n",
        "    \"\"\"모델 학습 및 평가\"\"\"\n",
        "\n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def prepare_train_test_split(self, feature_df, label_df, test_size=0.3):\n",
        "        \"\"\"시계열 기준 train/test 분할\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Train/Test 분할\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 피쳐와 라벨 병합\n",
        "        merged = feature_df.merge(\n",
        "            label_df[['Account', 'TimeUnit', 'Is Laundering']],\n",
        "            on=['Account', 'TimeUnit'],\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        # 시간순 정렬\n",
        "        merged = merged.sort_values('TimeUnit')\n",
        "\n",
        "        # 시간 기준 분할\n",
        "        split_idx = int(len(merged) * (1 - test_size))\n",
        "        train_df = merged.iloc[:split_idx]\n",
        "        test_df = merged.iloc[split_idx:]\n",
        "\n",
        "        print(f\"Train set: {len(train_df)} (자금세탁: {train_df['Is Laundering'].sum()})\")\n",
        "        print(f\"Test set: {len(test_df)} (자금세탁: {test_df['Is Laundering'].sum()})\")\n",
        "        print(f\"Train 자금세탁 비율: {train_df['Is Laundering'].mean():.4%}\")\n",
        "        print(f\"Test 자금세탁 비율: {test_df['Is Laundering'].mean():.4%}\")\n",
        "\n",
        "        # Feature와 Label 분리\n",
        "        feature_cols = [c for c in merged.columns\n",
        "                       if c not in ['Account', 'TimeUnit', 'Is Laundering']]\n",
        "\n",
        "        X_train = train_df[feature_cols]\n",
        "        y_train = train_df['Is Laundering']\n",
        "        X_test = test_df[feature_cols]\n",
        "        y_test = test_df['Is Laundering']\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, test_df\n",
        "\n",
        "    def train_baseline_model(self, X_train, y_train, X_test, y_test,\n",
        "                            use_smote=False, scale_pos_weight=None):\n",
        "        \"\"\"Baseline: XGBoost/CatBoost\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Baseline 모델 학습 (XGBoost)\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # SMOTE 적용 여부\n",
        "        if use_smote:\n",
        "            print(\"SMOTE 오버샘플링 적용 중...\")\n",
        "            smote = SMOTE(random_state=self.random_state)\n",
        "            X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "            print(f\"SMOTE 후 - Positive: {y_train_res.sum()}, Negative: {len(y_train_res) - y_train_res.sum()}\")\n",
        "        else:\n",
        "            X_train_res, y_train_res = X_train, y_train\n",
        "\n",
        "        # XGBoost 학습\n",
        "        if scale_pos_weight is None:\n",
        "            scale_pos_weight = (len(y_train) - y_train.sum()) / y_train.sum()\n",
        "\n",
        "        print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "        xgb_model = xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            random_state=self.random_state,\n",
        "            eval_metric='auc'\n",
        "        )\n",
        "\n",
        "        xgb_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "        # 예측\n",
        "        y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # 모델 저장\n",
        "        self.models['baseline_xgb'] = xgb_model\n",
        "\n",
        "        print(\"학습 완료!\")\n",
        "\n",
        "        return xgb_model, y_pred_proba\n",
        "\n",
        "    def evaluate_topk(self, y_true, y_pred_proba, test_df, k_values=[50, 100, 200, 500]):\n",
        "        \"\"\"Top-K 평가\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Top-K 평가\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # 점수 스케일링 (1000점 만점)\n",
        "        scores = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min()) * 1000\n",
        "\n",
        "        for k in k_values:\n",
        "            # Top K 선택\n",
        "            top_k_idx = np.argsort(y_pred_proba)[-k:]\n",
        "            y_pred_topk = np.zeros(len(y_true))\n",
        "            y_pred_topk[top_k_idx] = 1\n",
        "\n",
        "            # 메트릭 계산\n",
        "            precision = precision_score(y_true, y_pred_topk, zero_division=0)\n",
        "            recall = recall_score(y_true, y_pred_topk, zero_division=0)\n",
        "            f1 = f1_score(y_true, y_pred_topk, zero_division=0)\n",
        "\n",
        "            detected_laundering = y_true[top_k_idx].sum()\n",
        "            total_laundering = y_true.sum()\n",
        "\n",
        "            results[f'top_{k}'] = {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'detected': detected_laundering,\n",
        "                'total': total_laundering\n",
        "            }\n",
        "\n",
        "            print(f\"\\nTop-{k} 결과:\")\n",
        "            print(f\"  Precision: {precision:.4f}\")\n",
        "            print(f\"  Recall: {recall:.4f}\")\n",
        "            print(f\"  F1-Score: {f1:.4f}\")\n",
        "            print(f\"  탐지된 자금세탁: {detected_laundering}/{total_laundering}\")\n",
        "\n",
        "        # 점수 구간별 분포\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"점수 구간별 분포\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        bins = range(0, 1001, 100)\n",
        "        score_bins = pd.cut(scores, bins=bins, right=False)\n",
        "\n",
        "        for bin_range in score_bins.cat.categories:\n",
        "            mask = score_bins == bin_range\n",
        "            bin_positive = y_true[mask].sum()\n",
        "            bin_negative = len(y_true[mask]) - bin_positive\n",
        "\n",
        "            print(f\"{bin_range}: 정상={bin_negative}, 자금세탁={bin_positive}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def explain_with_shap(self, model, X_train, X_test, feature_names):\n",
        "        \"\"\"SHAP을 이용한 피쳐 중요도 분석\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"XAI: SHAP 피쳐 중요도 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        try:\n",
        "            import shap\n",
        "\n",
        "            # SHAP 값 계산\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_test.iloc[:100])  # 샘플만\n",
        "\n",
        "            # 피쳐 중요도\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': np.abs(shap_values).mean(axis=0)\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            print(\"\\nTop 20 중요 피쳐:\")\n",
        "            print(feature_importance.head(20))\n",
        "\n",
        "            return feature_importance\n",
        "        except ImportError:\n",
        "            print(\"SHAP 라이브러리가 설치되지 않았습니다. pip install shap\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 5. 메인 실행 파이프라인\n",
        "# ====================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"전체 파이프라인 실행\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"금융결제원 AML 탐지 프로젝트 시작\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # ========== 1. 데이터 로드 ==========\n",
        "    loader = AMLDataLoader(\n",
        "        trans_path='HI-Medium_Trans.csv',\n",
        "        accounts_path='HI-Medium_Accounts.csv'\n",
        "    )\n",
        "\n",
        "    df_trans, df_accounts = loader.load_data()\n",
        "    df_trans = loader.explore_data(df_trans, df_accounts)\n",
        "\n",
        "    # ========== 2. 전처리 및 샘플링 ==========\n",
        "    preprocessor = AMLPreprocessor(sample_ratio=0.1)\n",
        "\n",
        "    # 시간 단위 배치 샘플 생성\n",
        "    all_trans, account_time_labels = preprocessor.create_hourly_samples(df_trans)\n",
        "\n",
        "    # 자금세탁 패턴 분석\n",
        "    laundering, normal = preprocessor.analyze_laundering_patterns(df_trans)\n",
        "\n",
        "    # 샘플링 (큰 데이터를 작게)\n",
        "    sampled_labels = preprocessor.stratified_sample(account_time_labels)\n",
        "\n",
        "    # ========== 3. 피쳐 생성 ==========\n",
        "    feature_engineer = FeatureEngineer()\n",
        "\n",
        "    # 집계 피쳐 생성\n",
        "    print(\"\\n⚠️ 주의: 전체 데이터에 대해 피쳐를 생성하려면 시간이 오래 걸립니다.\")\n",
        "    print(\"시연을 위해 1000건만 생성합니다.\")\n",
        "    agg_features = feature_engineer.create_aggregation_features(\n",
        "        all_trans,\n",
        "        sampled_labels\n",
        "    )\n",
        "\n",
        "    # 그래프 피쳐 생성\n",
        "    graph_features = feature_engineer.create_graph_features(\n",
        "        all_trans,\n",
        "        sampled_labels\n",
        "    )\n",
        "\n",
        "    # 피쳐 병합\n",
        "    all_features = agg_features.merge(\n",
        "        graph_features,\n",
        "        on=['Account', 'TimeUnit'],\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    print(f\"\\n전체 피쳐 수: {len([c for c in all_features.columns if c not in ['Account', 'TimeUnit']])}\")\n",
        "\n",
        "    # ========== 4. 모델 학습 ==========\n",
        "    trainer = AMLModelTrainer()\n",
        "\n",
        "    # Train/Test 분할\n",
        "    X_train, X_test, y_train, y_test, test_df = trainer.prepare_train_test_split(\n",
        "        all_features,\n",
        "        sampled_labels\n",
        "    )\n",
        "\n",
        "    # Baseline 모델 학습\n",
        "    baseline_model, y_pred_proba = trainer.train_baseline_model(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        use_smote=False,  # SMOTE 사용 여부\n",
        "        scale_pos_weight=None  # Auto 계산\n",
        "    )\n",
        "\n",
        "    # ========== 5. 평가 ==========\n",
        "    # Top-K 평가\n",
        "    topk_results = trainer.evaluate_topk(\n",
        "        y_test.values,\n",
        "        y_pred_proba,\n",
        "        test_df,\n",
        "        k_values=[50, 100, 200]\n",
        "    )\n",
        "\n",
        "    # Feature Importance (XAI)\n",
        "    feature_names = [c for c in X_train.columns]\n",
        "    feature_importance = trainer.explain_with_shap(\n",
        "        baseline_model,\n",
        "        X_train,\n",
        "        X_test,\n",
        "        feature_names\n",
        "    )\n",
        "\n",
        "    # ========== 6. 결과 저장 ==========\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"프로젝트 완료!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n다음 단계:\")\n",
        "    print(\"1. 그래프 피쳐 추가 전후 성능 비교\")\n",
        "    print(\"2. 시계열 모델 앙상블\")\n",
        "    print(\"3. GNN 모델 적용\")\n",
        "    print(\"4. K-Fold Cross Validation으로 안정성 검증\")\n",
        "\n",
        "    return {\n",
        "        'baseline_model': baseline_model,\n",
        "        'topk_results': topk_results,\n",
        "        'feature_importance': feature_importance\n",
        "    }\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 추가: GNN 모델 (향후 구현)\n",
        "# ====================================\n",
        "\n",
        "class GNNModel:\n",
        "    \"\"\"Graph Neural Network for AML Detection\n",
        "    향후 구현 예정:\n",
        "    - GraphSAGE\n",
        "    - GAT (Graph Attention Network)\n",
        "    - Temporal GNN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"GNN 모델은 PyTorch Geometric 라이브러리 필요\")\n",
        "        print(\"pip install torch-geometric\")\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"GNN 모델 구축\"\"\"\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"GNN 학습\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 실행\n",
        "    results = main()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"사용 예시:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\"\"\n",
        "    # 1. 데이터 로드\n",
        "    loader = AMLDataLoader('HI-Medium_Trans.csv', 'HI-Medium_Accounts.csv')\n",
        "    df_trans, df_accounts = loader.load_data()\n",
        "\n",
        "    # 2. 전처리\n",
        "    preprocessor = AMLPreprocessor(sample_ratio=0.1)\n",
        "    all_trans, account_time_labels = preprocessor.create_hourly_samples(df_trans)\n",
        "\n",
        "    # 3. 피쳐 생성\n",
        "    feature_engineer = FeatureEngineer()\n",
        "    features = feature_engineer.create_aggregation_features(all_trans, account_time_labels)\n",
        "\n",
        "    # 4. 모델 학습\n",
        "    trainer = AMLModelTrainer()\n",
        "    X_train, X_test, y_train, y_test, test_df = trainer.prepare_train_test_split(features, account_time_labels)\n",
        "    model, predictions = trainer.train_baseline_model(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    # 5. 평가\n",
        "    results = trainer.evaluate_topk(y_test, predictions, test_df)\n",
        "    \"\"\")"
      ]
    }
  ]
}