{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kahram-y/AML_project/blob/main/AML(2_1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "금융결제원 AML 탐지 프로젝트\n",
        "\n",
        "IBM Transactions for Anti Money Laundering 데이터셋 활용\n",
        "\n",
        "목표: 자금세탁 계좌 탐지 (노드 분류)\n",
        "\n",
        "프로젝트 구조:\n",
        "1. 데이터 탐색 및 전처리\n",
        "2. 문제 정의\n",
        "3. 피쳐 생성 (집계 피쳐 + 그래프 피쳐)\n",
        "4. 모델 학습 및 평가\n",
        "   - Baseline: XGBoost/CatBoost\n",
        "   - 고도화 모델\n",
        "      - 시계열 모델 앙상블\n",
        "      - 그래프 피쳐 추가 전후 비교\n",
        "      - GNN 모델\n",
        "      - GNN 아키텍처 고도화\n",
        "      - 백테스트/시뮬레이션"
      ],
      "metadata": {
        "id": "30Iz9L_66xpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy scikit-learn xgboost catboost lightgbm networkx matplotlib seaborn imbalanced-learn shap"
      ],
      "metadata": {
        "id": "r8R6ZcAv6x-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb8d9e5-1867-4305-d93a-e08bb29e3e92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.3)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.1)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.50.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.5)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (0.1.5)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "import xgboost as xgb, XGBClassifier\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "tC91O4zh698v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 파일 경로\n",
        "trans_path = '/content/drive/MyDrive/HI-Small_Trans.csv'\n",
        "accounts_path = '/content/drive/MyDrive/HI-Small_accounts.csv'"
      ],
      "metadata": {
        "id": "riICG9oD6vzI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49429d17-0173-4c48-dc2a-4b87b27cca6d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "# 1. 데이터 로드 및 탐색\n",
        "# ====================================\n",
        "\n",
        "class AMLDataLoader:\n",
        "    \"\"\"AML 데이터 로드 및 초기 탐색\"\"\"\n",
        "\n",
        "    def __init__(self, trans_path, accounts_path):\n",
        "        self.trans_path = trans_path\n",
        "        self.accounts_path = accounts_path\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"데이터 로드\"\"\"\n",
        "        print(\"=\" * 80)\n",
        "        print(\"데이터 로딩 중...\")\n",
        "\n",
        "        # 거래 데이터 로드\n",
        "        self.transactions = pd.read_csv(self.trans_path)\n",
        "\n",
        "        # 계좌 데이터 로드\n",
        "        self.accounts = pd.read_csv(self.accounts_path)\n",
        "\n",
        "        print(f\"거래 데이터 shape: {self.transactions.shape}\")\n",
        "        print(f\"계좌 데이터 shape: {self.accounts.shape}\")\n",
        "\n",
        "        return self.transactions, self.accounts\n",
        "\n",
        "    def explore_data(self, df_trans, df_accounts):\n",
        "        \"\"\"데이터 탐색\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"데이터 탐색\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 거래 데이터 기본 정보\n",
        "        print(\"\\n[거래 데이터 샘플]\")\n",
        "        print(df_trans.head())\n",
        "        print(f\"\\n컬럼: {df_trans.columns.tolist()}\")\n",
        "        print(f\"\\n결측치:\\n{df_trans.isnull().sum()}\")\n",
        "\n",
        "        # Is Laundering 분포\n",
        "        if 'Is Laundering' in df_trans.columns:\n",
        "            laundering_dist = df_trans['Is Laundering'].value_counts()\n",
        "            print(f\"\\n[자금세탁 분포]\")\n",
        "            print(laundering_dist)\n",
        "            print(f\"자금세탁 비율: {laundering_dist[1] / len(df_trans) * 100:.4f}%\")\n",
        "\n",
        "        # 시간 정보 파싱 및 분포 확인\n",
        "        df_trans['Timestamp'] = pd.to_datetime(df_trans['Timestamp'])\n",
        "        df_trans['Year'] = df_trans['Timestamp'].dt.year\n",
        "        df_trans['Month'] = df_trans['Timestamp'].dt.month\n",
        "        df_trans['Day'] = df_trans['Timestamp'].dt.day\n",
        "        df_trans['Hour'] = df_trans['Timestamp'].dt.hour\n",
        "        df_trans['DayOfWeek'] = df_trans['Timestamp'].dt.dayofweek\n",
        "\n",
        "        print(f\"\\n[시간 범위]\")\n",
        "        print(f\"시작: {df_trans['Timestamp'].min()}\")\n",
        "        print(f\"종료: {df_trans['Timestamp'].max()}\")\n",
        "\n",
        "        # Day별 자금세탁 분포 확인\n",
        "        if 'Is Laundering' in df_trans.columns:\n",
        "            day_dist = df_trans.groupby('Day')['Is Laundering'].agg(['sum', 'count', 'mean'])\n",
        "            print(f\"\\n[일별 자금세탁 분포]\")\n",
        "            print(day_dist)\n",
        "\n",
        "            # 치우침 확인\n",
        "            if day_dist['mean'].std() > 0.1:\n",
        "                print(\"\\n⚠️ Day 피쳐의 label 분포가 치우쳐져 있습니다. 제거 고려 필요\")\n",
        "\n",
        "        # 계좌 데이터 정보\n",
        "        print(f\"\\n[계좌 데이터 샘플]\")\n",
        "        print(df_accounts.head())\n",
        "\n",
        "        # Bank Name 분포 확인\n",
        "        if 'Bank Name' in df_accounts.columns:\n",
        "            print(f\"\\n은행 분포:\\n{df_accounts['Bank Name'].value_counts().head(10)}\")\n",
        "\n",
        "        return df_trans\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 2. 데이터 전처리 및 샘플링\n",
        "# ====================================\n",
        "\n",
        "class AMLPreprocessor:\n",
        "    \"\"\"데이터 전처리 및 샘플링\"\"\"\n",
        "\n",
        "    def __init__(self, sample_ratio=0.1, random_state=42):\n",
        "        self.sample_ratio = sample_ratio\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def create_hourly_samples(self, df_trans):\n",
        "        \"\"\"시간 단위 배치로 모델 단위 생성\n",
        "        각 계좌번호 + 시간 단위로 샘플 생성\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"시간 단위 배치 샘플 생성\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # From Account 기준 샘플 생성\n",
        "        from_samples = df_trans.copy()\n",
        "        from_samples['Account'] = from_samples['From Bank'].astype(str) + '_' + from_samples['Account'].astype(str)\n",
        "        from_samples['TimeUnit'] = from_samples['Timestamp'].dt.strftime('%Y-%m-%d %H')\n",
        "        from_samples['Direction'] = 'OUT'\n",
        "\n",
        "        # To Account 기준 샘플 생성\n",
        "        to_samples = df_trans.copy()\n",
        "        to_samples['Account'] = to_samples['To Bank'].astype(str) + '_' + to_samples['Account.1'].astype(str)\n",
        "        to_samples['TimeUnit'] = to_samples['Timestamp'].dt.strftime('%Y-%m-%d %H')\n",
        "        to_samples['Direction'] = 'IN'\n",
        "\n",
        "        # 합치기\n",
        "        all_samples = pd.concat([from_samples, to_samples], ignore_index=True)\n",
        "\n",
        "        # 계좌 + 시간 단위로 그룹화하여 label 결정\n",
        "        # Is Laundering=1인 거래가 하나라도 있으면 해당 시간의 계좌는 suspicious\n",
        "        account_time_labels = all_samples.groupby(['Account', 'TimeUnit']).agg({\n",
        "            'Is Laundering': 'max',  # 하나라도 1이면 1\n",
        "            'Timestamp': 'min'\n",
        "        }).reset_index()\n",
        "\n",
        "        account_time_labels.rename(columns={'Timestamp': 'TimeUnit_Start'}, inplace=True)\n",
        "\n",
        "        print(f\"총 샘플 수 (계좌-시간 단위): {len(account_time_labels)}\")\n",
        "        print(f\"자금세탁 샘플: {account_time_labels['Is Laundering'].sum()}\")\n",
        "\n",
        "        return all_samples, account_time_labels\n",
        "\n",
        "    def stratified_sample(self, df, target_col='Is Laundering'):\n",
        "        \"\"\"계층화 샘플링\"\"\"\n",
        "        print(f\"\\n계층화 샘플링 (비율: {self.sample_ratio})\")\n",
        "\n",
        "        # 클래스별로 샘플링\n",
        "        sampled_dfs = []\n",
        "        for label in df[target_col].unique():\n",
        "            label_df = df[df[target_col] == label]\n",
        "            n_samples = int(len(label_df) * self.sample_ratio)\n",
        "            sampled = label_df.sample(n=n_samples, random_state=self.random_state)\n",
        "            sampled_dfs.append(sampled)\n",
        "\n",
        "        result = pd.concat(sampled_dfs, ignore_index=True)\n",
        "        print(f\"샘플링 후 크기: {len(result)}\")\n",
        "        print(f\"자금세탁 비율: {result[target_col].sum() / len(result) * 100:.4f}%\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def analyze_laundering_patterns(self, df_trans):\n",
        "        \"\"\"자금세탁 건과 정상 건의 차이 분석\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"자금세탁 패턴 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        laundering = df_trans[df_trans['Is Laundering'] == 1]\n",
        "        normal = df_trans[df_trans['Is Laundering'] == 0]\n",
        "\n",
        "        # 거래 금액 비교\n",
        "        print(f\"\\n[거래 금액 통계]\")\n",
        "        print(f\"자금세탁 - 평균: ${laundering['Amount Received'].mean():.2f}, \"\n",
        "              f\"중앙값: ${laundering['Amount Received'].median():.2f}\")\n",
        "        print(f\"정상 거래 - 평균: ${normal['Amount Received'].mean():.2f}, \"\n",
        "              f\"중앙값: ${normal['Amount Received'].median():.2f}\")\n",
        "\n",
        "        # 결제 수단 분포\n",
        "        print(f\"\\n[결제 수단 분포]\")\n",
        "        print(\"자금세탁:\")\n",
        "        print(laundering['Receiving Currency'].value_counts().head())\n",
        "        print(\"\\n정상 거래:\")\n",
        "        print(normal['Receiving Currency'].value_counts().head())\n",
        "\n",
        "        # 시간대 분포\n",
        "        print(f\"\\n[시간대 분포]\")\n",
        "        print(\"자금세탁 - 시간대별:\")\n",
        "        print(laundering['Hour'].value_counts().sort_index())\n",
        "\n",
        "        return laundering, normal\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 3. 피쳐 생성\n",
        "# ====================================\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"집계 피쳐 및 그래프 피쳐 생성\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "\n",
        "    def create_aggregation_features(self, df_trans, account_time_df):\n",
        "        \"\"\"50개 이상의 집계 피쳐 생성\n",
        "        과거 정보만 사용 (Data Leakage 방지)\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"집계 피쳐 생성\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        features_list = []\n",
        "        error_count = 0\n",
        "\n",
        "        # 샘플 수 제한 (시연용) - 증가\n",
        "        max_samples = min(2000, len(account_time_df))  # 1000 → 2000\n",
        "        print(f\"총 {len(account_time_df)}건 중 {max_samples}건 처리\")\n",
        "\n",
        "        # TimeUnit을 문자열로 통일\n",
        "        account_time_df['TimeUnit'] = account_time_df['TimeUnit'].astype(str)\n",
        "\n",
        "        for idx, row in account_time_df.head(max_samples).iterrows():\n",
        "            if idx % 200 == 0:\n",
        "                print(f\"진행: {idx}/{max_samples} (에러: {error_count}건)\")\n",
        "\n",
        "            try:\n",
        "                account = row['Account']\n",
        "                time_unit = pd.to_datetime(row['TimeUnit_Start'])\n",
        "                time_unit_str = row['TimeUnit']  # 문자열 버전 저장\n",
        "\n",
        "                # 해당 계좌의 과거 거래만 필터링\n",
        "                past_trans = df_trans[\n",
        "                    (df_trans['Account'] == account) &\n",
        "                    (df_trans['Timestamp'] < time_unit)\n",
        "                ].copy()\n",
        "\n",
        "                if len(past_trans) == 0:\n",
        "                    # 과거 거래가 없으면 기본값\n",
        "                    features = self._default_features(account, time_unit_str)\n",
        "                else:\n",
        "                    features = self._compute_features(past_trans, account, time_unit_str)\n",
        "\n",
        "                features_list.append(features)\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                if error_count <= 5:  # 처음 5개 에러만 출력\n",
        "                    print(f\"⚠️ 에러 발생 (idx={idx}): {e}\")\n",
        "\n",
        "                # 에러 발생 시에도 기본값 추가\n",
        "                try:\n",
        "                    features = self._default_features(row['Account'], row['TimeUnit'])\n",
        "                    features_list.append(features)\n",
        "                except:\n",
        "                    pass  # 기본값 생성도 실패하면 스킵\n",
        "\n",
        "        if len(features_list) == 0:\n",
        "            print(\"⚠️ 피쳐 생성 실패! 빈 데이터프레임 반환\")\n",
        "            # 최소한의 피쳐 구조 생성\n",
        "            return pd.DataFrame({\n",
        "                'Account': [],\n",
        "                'TimeUnit': [],\n",
        "                'total_trans_count': []\n",
        "            })\n",
        "\n",
        "        feature_df = pd.DataFrame(features_list)\n",
        "        self.feature_names = [c for c in feature_df.columns\n",
        "                             if c not in ['Account', 'TimeUnit']]\n",
        "\n",
        "        print(f\"\\n✅ 피쳐 생성 완료!\")\n",
        "        print(f\"  - 총 샘플: {len(feature_df)}건\")\n",
        "        print(f\"  - 생성된 피쳐 수: {len(self.feature_names)}개\")\n",
        "        print(f\"  - 에러 발생: {error_count}건\")\n",
        "        print(f\"  - 피쳐 목록: {self.feature_names[:100]}\")\n",
        "\n",
        "        return feature_df\n",
        "\n",
        "    def _compute_features(self, past_trans, account, time_unit_str):\n",
        "        \"\"\"실제 피쳐 계산\"\"\"\n",
        "\n",
        "        # time_unit_str를 datetime으로 변환\n",
        "        try:\n",
        "            time_unit = pd.to_datetime(time_unit_str)\n",
        "        except:\n",
        "            # 변환 실패 시 기본값 반환\n",
        "            return self._default_features(account, time_unit_str)\n",
        "\n",
        "        features = {'Account': account, 'TimeUnit': str(time_unit_str)}\n",
        "\n",
        "        # 시간 윈도우 정의\n",
        "        windows = {\n",
        "            '1h': timedelta(hours=1),\n",
        "            '3h': timedelta(hours=3),\n",
        "            '1d': timedelta(days=1),\n",
        "            '7d': timedelta(days=7)\n",
        "        }\n",
        "\n",
        "        for window_name, window_delta in windows.items():\n",
        "            window_start = time_unit - window_delta  # 이제 datetime 연산 가능\n",
        "            window_trans = past_trans[past_trans['Timestamp'] >= window_start]\n",
        "\n",
        "            # OUT 거래 (송금)\n",
        "            out_trans = window_trans[window_trans['Direction'] == 'OUT']\n",
        "            features[f'out_count_{window_name}'] = len(out_trans)\n",
        "\n",
        "            if len(out_trans) > 0:\n",
        "                try:\n",
        "                    out_amounts = out_trans['Amount Paid'].values\n",
        "                    features[f'out_amount_sum_{window_name}'] = float(np.sum(out_amounts))\n",
        "                    features[f'out_amount_mean_{window_name}'] = float(np.mean(out_amounts))\n",
        "                    features[f'out_amount_std_{window_name}'] = float(np.std(out_amounts)) if len(out_amounts) > 1 else 0.0\n",
        "                    features[f'out_amount_max_{window_name}'] = float(np.max(out_amounts))\n",
        "                    features[f'out_amount_min_{window_name}'] = float(np.min(out_amounts))\n",
        "                except Exception as e:\n",
        "                    # 에러 발생 시 기본값\n",
        "                    features[f'out_amount_sum_{window_name}'] = 0.0\n",
        "                    features[f'out_amount_mean_{window_name}'] = 0.0\n",
        "                    features[f'out_amount_std_{window_name}'] = 0.0\n",
        "                    features[f'out_amount_max_{window_name}'] = 0.0\n",
        "                    features[f'out_amount_min_{window_name}'] = 0.0\n",
        "            else:\n",
        "                features[f'out_amount_sum_{window_name}'] = 0.0\n",
        "                features[f'out_amount_mean_{window_name}'] = 0.0\n",
        "                features[f'out_amount_std_{window_name}'] = 0.0\n",
        "                features[f'out_amount_max_{window_name}'] = 0.0\n",
        "                features[f'out_amount_min_{window_name}'] = 0.0\n",
        "\n",
        "            # IN 거래 (입금)\n",
        "            in_trans = window_trans[window_trans['Direction'] == 'IN']\n",
        "            features[f'in_count_{window_name}'] = len(in_trans)\n",
        "\n",
        "            if len(in_trans) > 0:\n",
        "                try:\n",
        "                    in_amounts = in_trans['Amount Received'].values\n",
        "                    features[f'in_amount_sum_{window_name}'] = float(np.sum(in_amounts))\n",
        "                    features[f'in_amount_mean_{window_name}'] = float(np.mean(in_amounts))\n",
        "                    features[f'in_amount_std_{window_name}'] = float(np.std(in_amounts)) if len(in_amounts) > 1 else 0.0\n",
        "                    features[f'in_amount_max_{window_name}'] = float(np.max(in_amounts))\n",
        "                    features[f'in_amount_min_{window_name}'] = float(np.min(in_amounts))\n",
        "                except Exception as e:\n",
        "                    features[f'in_amount_sum_{window_name}'] = 0.0\n",
        "                    features[f'in_amount_mean_{window_name}'] = 0.0\n",
        "                    features[f'in_amount_std_{window_name}'] = 0.0\n",
        "                    features[f'in_amount_max_{window_name}'] = 0.0\n",
        "                    features[f'in_amount_min_{window_name}'] = 0.0\n",
        "            else:\n",
        "                features[f'in_amount_sum_{window_name}'] = 0.0\n",
        "                features[f'in_amount_mean_{window_name}'] = 0.0\n",
        "                features[f'in_amount_std_{window_name}'] = 0.0\n",
        "                features[f'in_amount_max_{window_name}'] = 0.0\n",
        "                features[f'in_amount_min_{window_name}'] = 0.0\n",
        "\n",
        "            # 순 흐름\n",
        "            features[f'net_flow_{window_name}'] = (\n",
        "                features[f'in_amount_sum_{window_name}'] -\n",
        "                features[f'out_amount_sum_{window_name}']\n",
        "            )\n",
        "\n",
        "            # 외화 거래\n",
        "            if len(window_trans) > 0:\n",
        "                try:\n",
        "                    foreign_curr = window_trans[\n",
        "                        window_trans['Payment Currency'] != window_trans['Receiving Currency']\n",
        "                    ]\n",
        "                    features[f'foreign_count_{window_name}'] = len(foreign_curr)\n",
        "                    features[f'foreign_ratio_{window_name}'] = len(foreign_curr) / len(window_trans)\n",
        "                except:\n",
        "                    features[f'foreign_count_{window_name}'] = 0\n",
        "                    features[f'foreign_ratio_{window_name}'] = 0.0\n",
        "            else:\n",
        "                features[f'foreign_count_{window_name}'] = 0\n",
        "                features[f'foreign_ratio_{window_name}'] = 0.0\n",
        "\n",
        "        # 전체 거래 통계\n",
        "        features['total_trans_count'] = len(past_trans)\n",
        "\n",
        "        try:\n",
        "            out_all = past_trans[past_trans['Direction'] == 'OUT']\n",
        "            in_all = past_trans[past_trans['Direction'] == 'IN']\n",
        "\n",
        "            if len(out_all) > 0:\n",
        "                features['total_out_amount'] = float(np.sum(out_all['Amount Paid'].values))\n",
        "            else:\n",
        "                features['total_out_amount'] = 0.0\n",
        "\n",
        "            if len(in_all) > 0:\n",
        "                features['total_in_amount'] = float(np.sum(in_all['Amount Received'].values))\n",
        "            else:\n",
        "                features['total_in_amount'] = 0.0\n",
        "        except:\n",
        "            features['total_out_amount'] = 0.0\n",
        "            features['total_in_amount'] = 0.0\n",
        "\n",
        "        # 거래 상대방 다양성\n",
        "        try:\n",
        "            unique_counterparties = set()\n",
        "\n",
        "            out_all = past_trans[past_trans['Direction'] == 'OUT']\n",
        "            if len(out_all) > 0:\n",
        "                for _, row in out_all.iterrows():\n",
        "                    counterparty = str(row['From Bank']) + '_' + str(row['Account'])\n",
        "                    unique_counterparties.add(counterparty)\n",
        "\n",
        "            in_all = past_trans[past_trans['Direction'] == 'IN']\n",
        "            if len(in_all) > 0:\n",
        "                for _, row in in_all.iterrows():\n",
        "                    counterparty = str(row['To Bank']) + '_' + str(row['Account.1'])\n",
        "                    unique_counterparties.add(counterparty)\n",
        "\n",
        "            features['unique_counterparties'] = len(unique_counterparties)\n",
        "        except:\n",
        "            features['unique_counterparties'] = 0\n",
        "\n",
        "        # 결제 수단 다양성\n",
        "        try:\n",
        "            features['unique_currencies'] = past_trans['Payment Currency'].nunique() if len(past_trans) > 0 else 0\n",
        "        except:\n",
        "            features['unique_currencies'] = 0\n",
        "\n",
        "        # 시간대 분포\n",
        "        try:\n",
        "            if len(past_trans) > 0:\n",
        "                hour_counts = past_trans['Hour'].value_counts()\n",
        "                night_mask = (hour_counts.index >= 0) & (hour_counts.index < 6)\n",
        "                night_count = hour_counts[night_mask].sum() if night_mask.any() else 0\n",
        "                features['night_trans_ratio'] = float(night_count) / len(past_trans)\n",
        "            else:\n",
        "                features['night_trans_ratio'] = 0.0\n",
        "        except:\n",
        "            features['night_trans_ratio'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _default_features(self, account, time_unit):\n",
        "        \"\"\"과거 거래가 없을 때 기본 피쳐\"\"\"\n",
        "        features = {'Account': account, 'TimeUnit': str(time_unit)}\n",
        "\n",
        "        # 모든 피쳐를 0으로 초기화\n",
        "        windows = ['1h', '3h', '1d', '7d']\n",
        "        for window in windows:\n",
        "            for prefix in ['out', 'in']:\n",
        "                for metric in ['count', 'amount_sum', 'amount_mean', 'amount_std', 'amount_max', 'amount_min']:\n",
        "                    features[f'{prefix}_{metric}_{window}'] = 0.0 if 'amount' in metric else 0\n",
        "            features[f'net_flow_{window}'] = 0.0\n",
        "            features[f'foreign_count_{window}'] = 0\n",
        "            features[f'foreign_ratio_{window}'] = 0.0\n",
        "\n",
        "        features['total_trans_count'] = 0\n",
        "        features['total_out_amount'] = 0.0\n",
        "        features['total_in_amount'] = 0.0\n",
        "        features['unique_counterparties'] = 0\n",
        "        features['unique_currencies'] = 0\n",
        "        features['night_trans_ratio'] = 0.0\n",
        "\n",
        "        return features\n",
        "\n",
        "    def create_graph_features(self, df_trans, account_time_df):\n",
        "        \"\"\"그래프 기반 피쳐 생성\n",
        "        - Centrality 기반 (Degree, Closeness, Betweenness)\n",
        "        - Path & Flow 패턴\n",
        "        - Community 구조\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"그래프 피쳐 생성\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        try:\n",
        "            # 거래 네트워크 구축\n",
        "            G = nx.DiGraph()\n",
        "\n",
        "            print(\"그래프 구축 중...\")\n",
        "            edge_count = 0\n",
        "            for idx, row in df_trans.iterrows():\n",
        "                try:\n",
        "                    from_acc = str(row['From Bank']) + '_' + str(row['Account'])\n",
        "                    to_acc = str(row['To Bank']) + '_' + str(row['Account.1'])\n",
        "\n",
        "                    # Amount Paid를 float으로 변환\n",
        "                    try:\n",
        "                        amount = float(row['Amount Paid'])\n",
        "                        if np.isnan(amount) or np.isinf(amount):\n",
        "                            amount = 0.0\n",
        "                    except (ValueError, TypeError):\n",
        "                        amount = 0.0\n",
        "\n",
        "                    if G.has_edge(from_acc, to_acc):\n",
        "                        G[from_acc][to_acc]['weight'] += amount\n",
        "                        G[from_acc][to_acc]['count'] += 1\n",
        "                    else:\n",
        "                        G.add_edge(from_acc, to_acc, weight=amount, count=1)\n",
        "\n",
        "                    edge_count += 1\n",
        "                except Exception as e:\n",
        "                    if edge_count < 5:\n",
        "                        print(f\"⚠️ 엣지 추가 실패: {e}\")\n",
        "                    continue\n",
        "\n",
        "            print(f\"그래프 구축 완료 - 노드: {G.number_of_nodes()}, 엣지: {G.number_of_edges()}\")\n",
        "\n",
        "            if G.number_of_nodes() == 0:\n",
        "                print(\"⚠️ 그래프가 비어있습니다. 기본 그래프 피쳐 반환\")\n",
        "                return self._default_graph_features(account_time_df)\n",
        "\n",
        "            # Centrality 계산\n",
        "            print(\"Centrality 계산 중...\")\n",
        "            degree_centrality = nx.degree_centrality(G)\n",
        "            in_degree_centrality = nx.in_degree_centrality(G)\n",
        "            out_degree_centrality = nx.out_degree_centrality(G)\n",
        "\n",
        "            # Betweenness는 계산량이 많으므로 샘플링\n",
        "            sample_size = min(500, G.number_of_nodes())\n",
        "            sample_nodes = list(G.nodes())[:sample_size]\n",
        "            betweenness_centrality = nx.betweenness_centrality(\n",
        "                G.subgraph(sample_nodes),\n",
        "                weight='weight'\n",
        "            )\n",
        "\n",
        "            # PageRank\n",
        "            try:\n",
        "                pagerank = nx.pagerank(G, weight='weight', max_iter=50)\n",
        "            except:\n",
        "                pagerank = {}\n",
        "\n",
        "            # 그래프 피쳐를 데이터프레임에 추가\n",
        "            graph_features = []\n",
        "\n",
        "            for _, row in account_time_df.iterrows():\n",
        "                account = row['Account']\n",
        "\n",
        "                features = {\n",
        "                    'Account': account,\n",
        "                    'TimeUnit': row['TimeUnit'],\n",
        "                    'degree_centrality': degree_centrality.get(account, 0.0),\n",
        "                    'in_degree_centrality': in_degree_centrality.get(account, 0.0),\n",
        "                    'out_degree_centrality': out_degree_centrality.get(account, 0.0),\n",
        "                    'betweenness_centrality': betweenness_centrality.get(account, 0.0),\n",
        "                    'pagerank': pagerank.get(account, 0.0),\n",
        "                }\n",
        "\n",
        "                # 이웃 노드 정보\n",
        "                try:\n",
        "                    if account in G:\n",
        "                        successors = list(G.successors(account))\n",
        "                        predecessors = list(G.predecessors(account))\n",
        "\n",
        "                        features['num_successors'] = len(successors)\n",
        "                        features['num_predecessors'] = len(predecessors)\n",
        "                        features['total_out_weight'] = float(sum(G[account][s]['weight'] for s in successors))\n",
        "                        features['total_in_weight'] = float(sum(G[p][account]['weight'] for p in predecessors))\n",
        "                    else:\n",
        "                        features['num_successors'] = 0\n",
        "                        features['num_predecessors'] = 0\n",
        "                        features['total_out_weight'] = 0.0\n",
        "                        features['total_in_weight'] = 0.0\n",
        "                except:\n",
        "                    features['num_successors'] = 0\n",
        "                    features['num_predecessors'] = 0\n",
        "                    features['total_out_weight'] = 0.0\n",
        "                    features['total_in_weight'] = 0.0\n",
        "\n",
        "                graph_features.append(features)\n",
        "\n",
        "            graph_feature_df = pd.DataFrame(graph_features)\n",
        "\n",
        "            print(f\"✅ 그래프 피쳐 생성 완료 - 피쳐 수: {len(graph_feature_df.columns) - 2}\")\n",
        "\n",
        "            return graph_feature_df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 그래프 피쳐 생성 실패: {e}\")\n",
        "            return self._default_graph_features(account_time_df)\n",
        "\n",
        "    def _default_graph_features(self, account_time_df):\n",
        "        \"\"\"그래프 피쳐 생성 실패 시 기본값 반환\"\"\"\n",
        "        graph_features = []\n",
        "\n",
        "        for _, row in account_time_df.iterrows():\n",
        "            features = {\n",
        "                'Account': row['Account'],\n",
        "                'TimeUnit': row['TimeUnit'],\n",
        "                'degree_centrality': 0.0,\n",
        "                'in_degree_centrality': 0.0,\n",
        "                'out_degree_centrality': 0.0,\n",
        "                'betweenness_centrality': 0.0,\n",
        "                'pagerank': 0.0,\n",
        "                'num_successors': 0,\n",
        "                'num_predecessors': 0,\n",
        "                'total_out_weight': 0.0,\n",
        "                'total_in_weight': 0.0\n",
        "            }\n",
        "            graph_features.append(features)\n",
        "\n",
        "        return pd.DataFrame(graph_features)\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 4. 모델 학습 및 평가\n",
        "# ====================================\n",
        "\n",
        "class AMLModelTrainer:\n",
        "    \"\"\"모델 학습 및 평가\"\"\"\n",
        "\n",
        "    def __init__(self, random_state=42):\n",
        "        self.random_state = random_state\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def prepare_train_test_split(self, feature_df, label_df, test_size=0.3):\n",
        "        \"\"\"시계열 기준 train/test 분할\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Train/Test 분할\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(f\"피쳐 데이터: {len(feature_df)}건\")\n",
        "        print(f\"라벨 데이터: {len(label_df)}건\")\n",
        "\n",
        "        # TimeUnit 형식 통일\n",
        "        feature_df['TimeUnit'] = feature_df['TimeUnit'].astype(str)\n",
        "        label_df['TimeUnit'] = label_df['TimeUnit'].astype(str)\n",
        "\n",
        "        print(f\"\\n피쳐 데이터 샘플:\")\n",
        "        print(feature_df[['Account', 'TimeUnit']].head(3))\n",
        "        print(f\"\\n라벨 데이터 샘플:\")\n",
        "        print(label_df[['Account', 'TimeUnit', 'Is Laundering']].head(3))\n",
        "\n",
        "        # 피쳐와 라벨 병합\n",
        "        merged = feature_df.merge(\n",
        "            label_df[['Account', 'TimeUnit', 'Is Laundering']],\n",
        "            on=['Account', 'TimeUnit'],\n",
        "            how='inner'\n",
        "        )\n",
        "\n",
        "        print(f\"\\n병합 후 데이터: {len(merged)}건\")\n",
        "\n",
        "        if len(merged) == 0:\n",
        "            print(\"⚠️ 경고: 병합 후 데이터가 0건입니다!\")\n",
        "            print(\"피쳐와 라벨의 키가 매칭되지 않습니다.\")\n",
        "            print(\"\\n피쳐 데이터의 Account 샘플:\")\n",
        "            print(feature_df['Account'].head(10).tolist())\n",
        "            print(\"\\n라벨 데이터의 Account 샘플:\")\n",
        "            print(label_df['Account'].head(10).tolist())\n",
        "\n",
        "            # 대안: 인덱스 기반 매칭 시도\n",
        "            print(\"\\n대안: 순서 기반 매칭 시도...\")\n",
        "            min_len = min(len(feature_df), len(label_df))\n",
        "            merged = feature_df.head(min_len).copy()\n",
        "            merged['Is Laundering'] = label_df.head(min_len)['Is Laundering'].values\n",
        "            print(f\"순서 기반 매칭 결과: {len(merged)}건\")\n",
        "\n",
        "        if len(merged) == 0:\n",
        "            raise ValueError(\"데이터 병합 실패! 피쳐와 라벨을 확인해주세요.\")\n",
        "\n",
        "        print(f\"자금세탁 비율: {merged['Is Laundering'].mean():.4%}\")\n",
        "\n",
        "        # 시간순 정렬\n",
        "        merged = merged.sort_values('TimeUnit')\n",
        "\n",
        "        # 시간 기준 분할\n",
        "        split_idx = int(len(merged) * (1 - test_size))\n",
        "\n",
        "        # 최소한 1건 이상은 보장\n",
        "        if split_idx == 0:\n",
        "            split_idx = max(1, len(merged) // 2)\n",
        "\n",
        "        train_df = merged.iloc[:split_idx]\n",
        "        test_df = merged.iloc[split_idx:]\n",
        "\n",
        "        print(f\"\\nTrain set: {len(train_df)} (자금세탁: {train_df['Is Laundering'].sum()})\")\n",
        "        print(f\"Test set: {len(test_df)} (자금세탁: {test_df['Is Laundering'].sum()})\")\n",
        "        print(f\"Train 자금세탁 비율: {train_df['Is Laundering'].mean():.4%}\")\n",
        "        print(f\"Test 자금세탁 비율: {test_df['Is Laundering'].mean():.4%}\")\n",
        "\n",
        "        # Feature와 Label 분리\n",
        "        feature_cols = [c for c in merged.columns\n",
        "                       if c not in ['Account', 'TimeUnit', 'Is Laundering']]\n",
        "\n",
        "        print(f\"\\n사용할 피쳐 수: {len(feature_cols)}개\")\n",
        "\n",
        "        X_train = train_df[feature_cols]\n",
        "        y_train = train_df['Is Laundering']\n",
        "        X_test = test_df[feature_cols]\n",
        "        y_test = test_df['Is Laundering']\n",
        "\n",
        "        return X_train, X_test, y_train, y_test, test_df\n",
        "\n",
        "    def train_baseline_model(self, X_train, y_train, X_test, y_test,\n",
        "                            use_smote=False, scale_pos_weight=None):\n",
        "        \"\"\"Baseline: XGBoost/CatBoost\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Baseline 모델 학습 (XGBoost)\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 데이터 검증\n",
        "        print(f\"학습 데이터: {len(X_train)}건\")\n",
        "        print(f\"테스트 데이터: {len(X_test)}건\")\n",
        "        print(f\"Positive 비율 (Train): {y_train.sum() / len(y_train) * 100:.4f}%\")\n",
        "        print(f\"Positive 비율 (Test): {y_test.sum() / len(y_test) * 100:.4f}%\")\n",
        "\n",
        "        # NaN 값 처리\n",
        "        X_train = X_train.fillna(0)\n",
        "        X_test = X_test.fillna(0)\n",
        "\n",
        "        # Inf 값 처리\n",
        "        X_train = X_train.replace([np.inf, -np.inf], 0)\n",
        "        X_test = X_test.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "        # SMOTE 적용 여부\n",
        "        if use_smote and y_train.sum() > 0:\n",
        "            print(\"SMOTE 오버샘플링 적용 중...\")\n",
        "            try:\n",
        "                smote = SMOTE(random_state=self.random_state)\n",
        "                X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
        "                print(f\"SMOTE 후 - Positive: {y_train_res.sum()}, Negative: {len(y_train_res) - y_train_res.sum()}\")\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ SMOTE 실패: {e}\")\n",
        "                print(\"원본 데이터로 학습합니다.\")\n",
        "                X_train_res, y_train_res = X_train, y_train\n",
        "        else:\n",
        "            X_train_res, y_train_res = X_train, y_train\n",
        "\n",
        "        # scale_pos_weight 계산\n",
        "        if scale_pos_weight is None:\n",
        "            if y_train_res.sum() > 0:\n",
        "                scale_pos_weight = (len(y_train_res) - y_train_res.sum()) / y_train_res.sum()\n",
        "            else:\n",
        "                print(\"⚠️ 학습 데이터에 Positive 샘플이 없습니다!\")\n",
        "                scale_pos_weight = 1.0\n",
        "\n",
        "        print(f\"scale_pos_weight: {scale_pos_weight:.2f}\")\n",
        "\n",
        "        # base_score 계산 (0과 1 사이로 제한)\n",
        "        positive_ratio = y_train_res.sum() / len(y_train_res)\n",
        "        base_score = max(0.01, min(0.99, positive_ratio))  # 0.01 ~ 0.99 범위로 제한\n",
        "\n",
        "        print(f\"base_score: {base_score:.4f}\")\n",
        "\n",
        "        # XGBoost 학습\n",
        "        xgb_model = xgb.XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            base_score=base_score,  # 명시적으로 설정\n",
        "            random_state=self.random_state,\n",
        "            eval_metric='logloss',  # auc 대신 logloss 사용\n",
        "            use_label_encoder=False\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            xgb_model.fit(X_train_res, y_train_res, verbose=False)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ XGBoost 학습 실패: {e}\")\n",
        "            print(\"CatBoost로 전환합니다...\")\n",
        "\n",
        "            # CatBoost로 대체\n",
        "            from catboost import CatBoostClassifier\n",
        "            xgb_model = CatBoostClassifier(\n",
        "                iterations=100,\n",
        "                depth=6,\n",
        "                learning_rate=0.1,\n",
        "                loss_function='Logloss',\n",
        "                random_state=self.random_state,\n",
        "                verbose=False\n",
        "            )\n",
        "            xgb_model.fit(X_train_res, y_train_res)\n",
        "\n",
        "        # 예측\n",
        "        y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # 모델 저장\n",
        "        self.models['baseline_xgb'] = xgb_model\n",
        "\n",
        "        print(\"학습 완료!\")\n",
        "\n",
        "        return xgb_model, y_pred_proba\n",
        "\n",
        "    def evaluate_topk(self, y_true, y_pred_proba, test_df, k_values=[50, 100, 200, 500]):\n",
        "        \"\"\"Top-K 평가\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"Top-K 평가\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 데이터 검증\n",
        "        if len(y_true) == 0:\n",
        "            print(\"⚠️ 테스트 데이터가 비어있습니다!\")\n",
        "            return {}\n",
        "\n",
        "        total_positive = y_true.sum()\n",
        "        print(f\"테스트 데이터: {len(y_true)}건\")\n",
        "        print(f\"자금세탁 건수: {total_positive}건 ({total_positive/len(y_true)*100:.4f}%)\")\n",
        "\n",
        "        if total_positive == 0:\n",
        "            print(\"⚠️ 테스트 데이터에 자금세탁 건이 없습니다!\")\n",
        "            print(\"이는 다음 원인일 수 있습니다:\")\n",
        "            print(\"  1. 샘플링 비율이 너무 낮음 (현재 10%)\")\n",
        "            print(\"  2. 시계열 분할 시 자금세탁 건이 모두 Train에 포함됨\")\n",
        "            print(\"  3. 데이터 필터링 과정에서 자금세탁 건이 제외됨\")\n",
        "            print(\"\\n해결 방법:\")\n",
        "            print(\"  - sample_ratio를 0.5 이상으로 높이기\")\n",
        "            print(\"  - test_size를 0.5로 조정하기\")\n",
        "            return {}\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # 점수 스케일링 (1000점 만점)\n",
        "        if len(np.unique(y_pred_proba)) > 1:\n",
        "            scores = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min()) * 1000\n",
        "        else:\n",
        "            scores = y_pred_proba * 1000\n",
        "\n",
        "        for k in k_values:\n",
        "            # K가 데이터보다 크면 조정\n",
        "            k_actual = min(k, len(y_true))\n",
        "\n",
        "            # Top K 선택\n",
        "            top_k_idx = np.argsort(y_pred_proba)[-k_actual:]\n",
        "            y_pred_topk = np.zeros(len(y_true))\n",
        "            y_pred_topk[top_k_idx] = 1\n",
        "\n",
        "            # 메트릭 계산\n",
        "            precision = precision_score(y_true, y_pred_topk, zero_division=0)\n",
        "            recall = recall_score(y_true, y_pred_topk, zero_division=0)\n",
        "            f1 = f1_score(y_true, y_pred_topk, zero_division=0)\n",
        "\n",
        "            detected_laundering = y_true[top_k_idx].sum()\n",
        "\n",
        "            results[f'top_{k}'] = {\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'detected': int(detected_laundering),\n",
        "                'total': int(total_positive)\n",
        "            }\n",
        "\n",
        "            print(f\"\\nTop-{k_actual} 결과:\")\n",
        "            print(f\"  Precision: {precision:.4f}\")\n",
        "            print(f\"  Recall: {recall:.4f}\")\n",
        "            print(f\"  F1-Score: {f1:.4f}\")\n",
        "            print(f\"  탐지된 자금세탁: {int(detected_laundering)}/{int(total_positive)}\")\n",
        "\n",
        "        # 점수 구간별 분포\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"점수 구간별 분포\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        try:\n",
        "            bins = list(range(0, 1001, 100))\n",
        "            score_bins = pd.cut(scores, bins=bins, right=False)\n",
        "\n",
        "            # Categorical의 categories 직접 접근\n",
        "            for bin_range in score_bins.categories:\n",
        "                mask = score_bins == bin_range\n",
        "                if mask.sum() > 0:\n",
        "                    bin_positive = y_true[mask].sum()\n",
        "                    bin_negative = mask.sum() - bin_positive\n",
        "\n",
        "                    print(f\"{bin_range}: 정상={int(bin_negative)}, 자금세탁={int(bin_positive)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 점수 구간별 분포 계산 실패: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def explain_with_shap(self, model, X_train, X_test, feature_names):\n",
        "        \"\"\"SHAP을 이용한 피쳐 중요도 분석\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"XAI: SHAP 피쳐 중요도 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        try:\n",
        "            import shap\n",
        "\n",
        "            # SHAP 값 계산\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            shap_values = explainer.shap_values(X_test.iloc[:100])  # 샘플만\n",
        "\n",
        "            # 피쳐 중요도\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'feature': feature_names,\n",
        "                'importance': np.abs(shap_values).mean(axis=0)\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            print(\"\\nTop 20 중요 피쳐:\")\n",
        "            print(feature_importance.head(20))\n",
        "\n",
        "            return feature_importance\n",
        "        except ImportError:\n",
        "            print(\"SHAP 라이브러리가 설치되지 않았습니다. pip install shap\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 5. 메인 실행 파이프라인\n",
        "# ====================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"전체 파이프라인 실행\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"금융결제원 AML 탐지 프로젝트 시작\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # ========== 1. 기본 데이터 로드 및 전처리 ==========\n",
        "    loader = AMLDataLoader(\n",
        "        trans_path=trans_path,\n",
        "        accounts_path=accounts_path\n",
        "    )\n",
        "\n",
        "    df_trans, df_accounts = loader.load_data()\n",
        "    df_trans = loader.explore_data(df_trans, df_accounts)\n",
        "\n",
        "    # 샘플링 비율을 높여서 더 많은 데이터 확보\n",
        "    preprocessor = AMLPreprocessor(sample_ratio=0.3)    # 10% → 30%로 증가\n",
        "\n",
        "    # 시간 단위 배치 샘플 생성\n",
        "    all_trans, account_time_labels = preprocessor.create_hourly_samples(df_trans)\n",
        "    sampled_labels = preprocessor.stratified_sample(account_time_labels)\n",
        "\n",
        "    print(f\"\\n전체 계좌-시간 샘플: {len(account_time_labels)}건\")\n",
        "    print(f\"자금세탁 샘플: {account_time_labels['Is Laundering'].sum()}건\")\n",
        "\n",
        "    # 자금세탁 패턴 분석\n",
        "    laundering, normal = preprocessor.analyze_laundering_patterns(df_trans)\n",
        "\n",
        "    # 샘플링 (큰 데이터를 작게)\n",
        "    sampled_labels = preprocessor.stratified_sample(account_time_labels)\n",
        "\n",
        "    print(f\"\\n샘플링 후:\")\n",
        "    print(f\"  총 샘플: {len(sampled_labels)}건\")\n",
        "    print(f\"  자금세탁: {sampled_labels['Is Laundering'].sum()}건\")\n",
        "    print(f\"  자금세탁 비율: {sampled_labels['Is Laundering'].mean():.4%}\")\n",
        "\n",
        "    # 자금세탁 건이 너무 적으면 경고\n",
        "    if sampled_labels['Is Laundering'].sum() < 10:\n",
        "        print(\"\\n⚠️ 경고: 자금세탁 샘플이 너무 적습니다!\")\n",
        "        print(\"sample_ratio를 더 높이거나 전체 데이터를 사용하세요.\")\n",
        "        print(\"일단 전체 데이터로 진행합니다...\")\n",
        "        sampled_labels = account_time_labels\n",
        "\n",
        "    # ========== 2. 피쳐 생성 ==========\n",
        "    feature_engineer = FeatureEngineer()\n",
        "\n",
        "    # 집계 피쳐 생성 (더 많은 샘플 처리)\n",
        "    print(\"\\n⚠️ 주의: 피쳐 생성에 시간이 걸릴 수 있습니다.\")\n",
        "    print(f\"최대 {min(2000, len(sampled_labels))}건 처리\")\n",
        "\n",
        "    # 집계 피쳐 (그래프 피쳐 제외)\n",
        "    agg_features = feature_engineer.create_aggregation_features(\n",
        "        all_trans,\n",
        "        sampled_labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\n집계 피쳐 생성 완료: {len(agg_features)}건\")\n",
        "\n",
        "    # 그래프 피쳐 생성\n",
        "    graph_features = feature_engineer.create_graph_features(\n",
        "        all_trans,\n",
        "        sampled_labels.head(len(agg_features))    # 집계 피쳐와 동일한 수만큼\n",
        "    )\n",
        "\n",
        "    print(f\"그래프 피쳐 생성 완료: {len(graph_features)}건\")\n",
        "\n",
        "    # 그래프 피쳐명 추출\n",
        "    graph_feature_names = [c for c in graph_features.columns\n",
        "                          if c not in ['Account', 'TimeUnit']]\n",
        "\n",
        "    print(f\"\\n그래프 피쳐 수: {len(graph_feature_names)}개\")\n",
        "    print(f\"그래프 피쳐: {graph_feature_names}\")\n",
        "\n",
        "    # 피쳐 병합 (그래프 포함)\n",
        "    all_features = agg_features.merge(\n",
        "        graph_features,\n",
        "        on=['Account', 'TimeUnit'],\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    print(f\"\\n피쳐 병합 후: {len(all_features)}건\")\n",
        "    print(f\"전체 피쳐 수: {len([c for c in all_features.columns if c not in ['Account', 'TimeUnit']])}\")\n",
        "\n",
        "    # 그래프 피쳐 제외 버전\n",
        "    features_no_graph = agg_features.copy()\n",
        "\n",
        "    # ========== 3.모델 학습 ==========\n",
        "    trainer = AMLModelTrainer()\n",
        "\n",
        "    # Train/Test 분할 (test_size를 크게 해서 테스트 데이터 확보)\n",
        "    # 그래프 피쳐 포함 버전\n",
        "    X_train, X_test, y_train, y_test, test_df = trainer.prepare_train_test_split(\n",
        "        all_features,\n",
        "        sampled_labels,\n",
        "        test_size=0.4   # 30% → 40%로 증가\n",
        "    )\n",
        "    # 그래프 피쳐 제외 버전\n",
        "    X_train_no_graph, X_test_no_graph, _, _, _ = trainer.prepare_train_test_split(\n",
        "        features_no_graph,\n",
        "        sampled_labels,\n",
        "        test_size=0.4\n",
        "    )\n",
        "\n",
        "    # 데이터가 없으면 중단\n",
        "    if len(X_train) == 0 or len(X_test) == 0:\n",
        "        print(\"\\n⚠️ 학습 또는 테스트 데이터가 없습니다. 프로세스를 중단합니다.\")\n",
        "        print(\"\\n문제 해결 방법:\")\n",
        "        print(\"1. sample_ratio를 1.0으로 설정 (전체 데이터 사용)\")\n",
        "        print(\"2. 피쳐 생성의 max_samples 제한 제거\")\n",
        "        print(\"3. 데이터 파일 경로 확인\")\n",
        "        return None\n",
        "\n",
        "    # Baseline 모델 학습\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Baseline 모델 학습\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    baseline_model, baseline_pred = trainer.train_baseline_model(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        use_smote=False,  # SMOTE 사용 여부\n",
        "        scale_pos_weight=None  # Auto 계산\n",
        "    )\n",
        "\n",
        "    # ========== 5. 평가 ==========\n",
        "    # Top-K 평가\n",
        "    topk_results = trainer.evaluate_topk(\n",
        "        y_test.values,\n",
        "        y_pred_proba,\n",
        "        test_df,\n",
        "        k_values=[10, 20, 50]  # K 값을 작게 조정\n",
        "    )\n",
        "\n",
        "    # Feature Importance (XAI)\n",
        "    if len(X_train) > 0 and len(X_test) > 0:\n",
        "        feature_names = [c for c in X_train.columns]\n",
        "        feature_importance = trainer.explain_with_shap(\n",
        "            baseline_model,\n",
        "            X_train,\n",
        "            X_test,\n",
        "            feature_names\n",
        "        )\n",
        "    else:\n",
        "        feature_importance = None\n",
        "\n",
        "    # ========== 5. 고도화 실험 실행 ==========\n",
        "    # 실험 프레임워크 초기화\n",
        "    exp_framework = ExperimentFramework()\n",
        "\n",
        "    # ---------- 실험 1: 그래프 피쳐 없음 ----------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"실험 1: Baseline (그래프 피쳐 제외)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    def train_no_graph(X_tr, y_tr, X_te, y_te):\n",
        "        from xgboost import XGBClassifier\n",
        "        scale_pos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum() if y_tr.sum() > 0 else 1.0\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        pred = model.predict_proba(X_te)[:, 1]\n",
        "        return model, pred\n",
        "\n",
        "    model_no_graph, pred_no_graph = exp_framework.run_experiment(\n",
        "        \"Baseline (No Graph)\",\n",
        "        train_no_graph,\n",
        "        X_train_no_graph, y_train,\n",
        "        X_test_no_graph, y_test\n",
        "    )\n",
        "\n",
        "    # ---------- 실험 2: 그래프 피쳐 포함 ----------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"실험 2: 그래프 피쳐 포함\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    def train_with_graph(X_tr, y_tr, X_te, y_te):\n",
        "        from xgboost import XGBClassifier\n",
        "        scale_pos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum() if y_tr.sum() > 0 else 1.0\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        pred = model.predict_proba(X_te)[:, 1]\n",
        "        return model, pred\n",
        "\n",
        "    model_with_graph, pred_with_graph = exp_framework.run_experiment(\n",
        "        \"With Graph Features\",\n",
        "        train_with_graph,\n",
        "        X_train, y_train,\n",
        "        X_test, y_test\n",
        "    )\n",
        "\n",
        "\n",
        "    # ========== 6. 결과 저장 ==========\n",
        "    print(\"\\n다음 단계:\")\n",
        "    print(\"1. 그래프 피쳐 추가 전후 성능 비교\")\n",
        "    print(\"1. 시계열 모델 앙상블\")\n",
        "    print(\"2. GNN 모델 적용\")\n",
        "    print(\"3. 백테스트\")\n",
        "\n",
        "    return {\n",
        "        'baseline_model': baseline_model,\n",
        "        'topk_results': topk_results,\n",
        "        'feature_importance': feature_importance\n",
        "    }\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 추가: GNN 모델 (향후 구현)\n",
        "# ====================================\n",
        "\n",
        "class GNNModel:\n",
        "    \"\"\"Graph Neural Network for AML Detection\n",
        "    향후 구현 예정:\n",
        "    - GraphSAGE\n",
        "    - GAT (Graph Attention Network)\n",
        "    - Temporal GNN\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"GNN 모델은 PyTorch Geometric 라이브러리 필요\")\n",
        "        print(\"pip install torch-geometric\")\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"GNN 모델 구축\"\"\"\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"GNN 학습\"\"\"\n",
        "        pass\n",
        "\n",
        "\"\"\"\n",
        "AML 탐지 프로젝트 - 고도화 모델 및 성능 비교\n",
        "1. 시계열 모델 앙상블\n",
        "2. 그래프 피쳐 추가 전후 비교\n",
        "3. GNN 모델 적용\n",
        "4. GNN 아키텍처 고도화\n",
        "5. 백테스트/시뮬레이션\n",
        "\"\"\"\n",
        "\n",
        "# ====================================\n",
        "# 1. 시계열 모델 앙상블\n",
        "# ====================================\n",
        "\n",
        "class TimeSeriesEnsemble:\n",
        "    \"\"\"시계열 특성을 다루는 모델과 Boosting 모델 앙상블\"\"\"\n",
        "\n",
        "    def __init__(self, base_model, random_state=42):\n",
        "        self.base_model = base_model  # XGBoost/CatBoost\n",
        "        self.lstm_model = None\n",
        "        self.ensemble_weights = {'base': 0.7, 'lstm': 0.3}\n",
        "        self.random_state = random_state\n",
        "\n",
        "    def build_lstm_model(self, input_dim, hidden_dim=64):\n",
        "        \"\"\"LSTM 기반 시계열 모델 구축\"\"\"\n",
        "\n",
        "        class LSTMAMLDetector(nn.Module):\n",
        "            def __init__(self, input_dim, hidden_dim):\n",
        "                super(LSTMAMLDetector, self).__init__()\n",
        "                self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2)\n",
        "                self.dropout = nn.Dropout(0.3)\n",
        "                self.fc1 = nn.Linear(hidden_dim, 32)\n",
        "                self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "            def forward(self, x):\n",
        "                # x: (batch, seq_len, features)\n",
        "                lstm_out, (h_n, c_n) = self.lstm(x)\n",
        "                # 마지막 시점의 hidden state 사용\n",
        "                out = self.dropout(h_n[-1])\n",
        "                out = F.relu(self.fc1(out))\n",
        "                out = torch.sigmoid(self.fc2(out))\n",
        "                return out\n",
        "\n",
        "        return LSTMAMLDetector(input_dim, hidden_dim)\n",
        "\n",
        "    def prepare_sequence_data(self, X, window_size=5):\n",
        "        \"\"\"시계열 윈도우 데이터 준비\"\"\"\n",
        "        sequences = []\n",
        "\n",
        "        # 계좌별로 시계열 윈도우 생성\n",
        "        for i in range(len(X) - window_size + 1):\n",
        "            seq = X[i:i+window_size]\n",
        "            sequences.append(seq)\n",
        "\n",
        "        return np.array(sequences)\n",
        "\n",
        "    def train_lstm(self, X_train, y_train, epochs=20, batch_size=32):\n",
        "        \"\"\"LSTM 모델 학습\"\"\"\n",
        "        print(\"\\n시계열 LSTM 모델 학습 중...\")\n",
        "\n",
        "        # 시퀀스 데이터 준비\n",
        "        X_seq = self.prepare_sequence_data(X_train.values)\n",
        "        y_seq = y_train.values[len(y_train) - len(X_seq):]\n",
        "\n",
        "        # PyTorch 텐서 변환\n",
        "        X_tensor = torch.FloatTensor(X_seq)\n",
        "        y_tensor = torch.FloatTensor(y_seq).reshape(-1, 1)\n",
        "\n",
        "        # 모델 초기화\n",
        "        self.lstm_model = self.build_lstm_model(X_train.shape[1])\n",
        "        optimizer = torch.optim.Adam(self.lstm_model.parameters(), lr=0.001)\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # 학습\n",
        "        self.lstm_model.train()\n",
        "        for epoch in range(epochs):\n",
        "            for i in range(0, len(X_tensor), batch_size):\n",
        "                batch_X = X_tensor[i:i+batch_size]\n",
        "                batch_y = y_tensor[i:i+batch_size]\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.lstm_model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(\"LSTM 학습 완료!\")\n",
        "\n",
        "    def predict_ensemble(self, X_test):\n",
        "        \"\"\"앙상블 예측\"\"\"\n",
        "        # Base model 예측\n",
        "        base_pred = self.base_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        # LSTM 예측\n",
        "        if self.lstm_model is not None:\n",
        "            X_seq = self.prepare_sequence_data(X_test.values)\n",
        "            X_tensor = torch.FloatTensor(X_seq)\n",
        "\n",
        "            self.lstm_model.eval()\n",
        "            with torch.no_grad():\n",
        "                lstm_pred_full = self.lstm_model(X_tensor).numpy().flatten()\n",
        "\n",
        "            # 길이 맞추기\n",
        "            lstm_pred = np.zeros(len(X_test))\n",
        "            lstm_pred[-len(lstm_pred_full):] = lstm_pred_full\n",
        "            lstm_pred[:len(X_test)-len(lstm_pred_full)] = lstm_pred_full[0]\n",
        "        else:\n",
        "            lstm_pred = base_pred\n",
        "\n",
        "        # 가중 평균 앙상블\n",
        "        ensemble_pred = (\n",
        "            self.ensemble_weights['base'] * base_pred +\n",
        "            self.ensemble_weights['lstm'] * lstm_pred\n",
        "        )\n",
        "\n",
        "        return ensemble_pred\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 2. 그래프 피쳐 추가 전후 비교\n",
        "# ====================================\n",
        "\n",
        "class GraphFeatureComparison:\n",
        "    \"\"\"그래프 피쳐 추가 전후 성능 비교\"\"\"\n",
        "\n",
        "    def __init__(self, model_class):\n",
        "        self.model_class = model_class\n",
        "        self.results = {}\n",
        "\n",
        "    def train_without_graph_features(self, X_train, y_train, X_test, y_test, graph_feature_names):\n",
        "        \"\"\"그래프 피쳐 없이 학습\"\"\"\n",
        "        print(\"\\n[실험 1] 그래프 피쳐 제외 모델\")\n",
        "\n",
        "        # 그래프 피쳐 제거\n",
        "        non_graph_cols = [c for c in X_train.columns if c not in graph_feature_names]\n",
        "        X_train_no_graph = X_train[non_graph_cols]\n",
        "        X_test_no_graph = X_test[non_graph_cols]\n",
        "\n",
        "        print(f\"사용 피쳐 수: {len(non_graph_cols)}개\")\n",
        "\n",
        "        # 모델 학습\n",
        "        model = self.model_class(random_state=42)\n",
        "        model.fit(X_train_no_graph, y_train)\n",
        "\n",
        "        # 예측 및 평가\n",
        "        y_pred_proba = model.predict_proba(X_test_no_graph)[:, 1]\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "        metrics = self._calculate_metrics(y_test, y_pred, y_pred_proba)\n",
        "        self.results['without_graph'] = metrics\n",
        "\n",
        "        return model, y_pred_proba\n",
        "\n",
        "    def train_with_graph_features(self, X_train, y_train, X_test, y_test):\n",
        "        \"\"\"그래프 피쳐 포함 학습\"\"\"\n",
        "        print(\"\\n[실험 2] 그래프 피쳐 포함 모델\")\n",
        "        print(f\"사용 피쳐 수: {X_train.shape[1]}개\")\n",
        "\n",
        "        # 모델 학습\n",
        "        model = self.model_class(random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # 예측 및 평가\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "        metrics = self._calculate_metrics(y_test, y_pred, y_pred_proba)\n",
        "        self.results['with_graph'] = metrics\n",
        "\n",
        "        return model, y_pred_proba\n",
        "\n",
        "    def _calculate_metrics(self, y_true, y_pred, y_pred_proba):\n",
        "        \"\"\"성능 지표 계산\"\"\"\n",
        "        return {\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'auc': roc_auc_score(y_true, y_pred_proba) if len(np.unique(y_true)) > 1 else 0\n",
        "        }\n",
        "\n",
        "    def compare_results(self):\n",
        "        \"\"\"결과 비교 출력\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"그래프 피쳐 추가 효과 비교\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        df_results = pd.DataFrame(self.results).T\n",
        "        print(df_results)\n",
        "\n",
        "        # 개선율 계산\n",
        "        if 'without_graph' in self.results and 'with_graph' in self.results:\n",
        "            improvement = {}\n",
        "            for metric in ['precision', 'recall', 'f1', 'auc']:\n",
        "                without = self.results['without_graph'][metric]\n",
        "                with_g = self.results['with_graph'][metric]\n",
        "                if without > 0:\n",
        "                    improvement[metric] = ((with_g - without) / without) * 100\n",
        "                else:\n",
        "                    improvement[metric] = 0\n",
        "\n",
        "            print(\"\\n개선율 (%):\")\n",
        "            for metric, value in improvement.items():\n",
        "                print(f\"  {metric}: {value:+.2f}%\")\n",
        "\n",
        "        return df_results\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 3. 간소화된 GNN 모델\n",
        "# ====================================\n",
        "\n",
        "class SimpleGNN:\n",
        "    \"\"\"PyTorch Geometric 없이 구현한 간소화 GNN\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=64, output_dim=1):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"GNN 모델 구축\"\"\"\n",
        "\n",
        "        class SimpleGCN(nn.Module):\n",
        "            def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "                super(SimpleGCN, self).__init__()\n",
        "                self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "                self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "                self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "                self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "            def forward(self, x, adj):\n",
        "                # x: 노드 피쳐 (N, input_dim)\n",
        "                # adj: 인접 행렬 (N, N)\n",
        "\n",
        "                # 첫 번째 GCN 레이어\n",
        "                x = torch.mm(adj, x)  # 이웃 정보 집계\n",
        "                x = self.fc1(x)\n",
        "                x = F.relu(x)\n",
        "                x = self.dropout(x)\n",
        "\n",
        "                # 두 번째 GCN 레이어\n",
        "                x = torch.mm(adj, x)\n",
        "                x = self.fc2(x)\n",
        "                x = F.relu(x)\n",
        "                x = self.dropout(x)\n",
        "\n",
        "                # 출력 레이어\n",
        "                x = self.fc3(x)\n",
        "                x = torch.sigmoid(x)\n",
        "\n",
        "                return x\n",
        "\n",
        "        return SimpleGCN(self.input_dim, self.hidden_dim, self.output_dim)\n",
        "\n",
        "    def build_adjacency_matrix(self, edge_list, num_nodes):\n",
        "        \"\"\"인접 행렬 생성\"\"\"\n",
        "        adj = np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "        for src, dst in edge_list:\n",
        "            adj[src, dst] = 1\n",
        "            # adj[dst, src] = 1  # 무방향 그래프인 경우\n",
        "\n",
        "        # Self-loop 추가\n",
        "        adj = adj + np.eye(num_nodes)\n",
        "\n",
        "        # 정규화 (D^-1/2 * A * D^-1/2)\n",
        "        rowsum = np.array(adj.sum(1))\n",
        "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "        d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
        "        adj_normalized = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)\n",
        "\n",
        "        return torch.FloatTensor(adj_normalized)\n",
        "\n",
        "    def train_model(self, X, y, edge_list, epochs=50, lr=0.01):\n",
        "        \"\"\"GNN 학습\"\"\"\n",
        "        print(\"\\nGNN 모델 학습 중...\")\n",
        "\n",
        "        # 데이터 준비\n",
        "        num_nodes = len(X)\n",
        "        X_tensor = torch.FloatTensor(X)\n",
        "        y_tensor = torch.FloatTensor(y.values).reshape(-1, 1)\n",
        "        adj_matrix = self.build_adjacency_matrix(edge_list, num_nodes)\n",
        "\n",
        "        # 옵티마이저 및 손실함수\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # 학습\n",
        "        self.model.train()\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(X_tensor, adj_matrix)\n",
        "            loss = criterion(outputs, y_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        print(\"GNN 학습 완료!\")\n",
        "\n",
        "    def predict(self, X, edge_list):\n",
        "        \"\"\"GNN 예측\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        num_nodes = len(X)\n",
        "        X_tensor = torch.FloatTensor(X)\n",
        "        adj_matrix = self.build_adjacency_matrix(edge_list, num_nodes)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor, adj_matrix)\n",
        "\n",
        "        return outputs.numpy().flatten()\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 4. GNN 아키텍처 고도화\n",
        "# ====================================\n",
        "\n",
        "class AdvancedGNN:\n",
        "    \"\"\"고도화된 GNN 아키텍처\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def build_gnn_rnn_hybrid(self):\n",
        "        \"\"\"GNN + RNN 하이브리드 모델\"\"\"\n",
        "\n",
        "        class GNN_RNN_Hybrid(nn.Module):\n",
        "            def __init__(self, input_dim, hidden_dim):\n",
        "                super(GNN_RNN_Hybrid, self).__init__()\n",
        "\n",
        "                # GNN 부분\n",
        "                self.gcn1 = nn.Linear(input_dim, hidden_dim)\n",
        "                self.gcn2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "                # RNN 부분 (시계열)\n",
        "                self.lstm = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "                # 융합 레이어\n",
        "                self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "                self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "                self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "            def forward(self, x, adj, x_seq=None):\n",
        "                # GNN 경로\n",
        "                h_gcn = torch.mm(adj, x)\n",
        "                h_gcn = F.relu(self.gcn1(h_gcn))\n",
        "                h_gcn = torch.mm(adj, h_gcn)\n",
        "                h_gcn = F.relu(self.gcn2(h_gcn))\n",
        "\n",
        "                # RNN 경로 (시계열이 있는 경우)\n",
        "                if x_seq is not None:\n",
        "                    _, (h_rnn, _) = self.lstm(x_seq)\n",
        "                    h_rnn = h_rnn[-1]\n",
        "                else:\n",
        "                    h_rnn = torch.zeros_like(h_gcn)\n",
        "\n",
        "                # 융합\n",
        "                h_combined = torch.cat([h_gcn, h_rnn], dim=1)\n",
        "                h_combined = self.dropout(h_combined)\n",
        "                h_combined = F.relu(self.fc1(h_combined))\n",
        "                output = torch.sigmoid(self.fc2(h_combined))\n",
        "\n",
        "                return output\n",
        "\n",
        "        return GNN_RNN_Hybrid(self.input_dim, self.hidden_dim)\n",
        "\n",
        "    def build_temporal_gnn(self):\n",
        "        \"\"\"Temporal GNN (TGN)\"\"\"\n",
        "\n",
        "        class TemporalGNN(nn.Module):\n",
        "            def __init__(self, input_dim, hidden_dim):\n",
        "                super(TemporalGNN, self).__init__()\n",
        "\n",
        "                # 시간별 GNN 레이어\n",
        "                self.temporal_gcn = nn.ModuleList([\n",
        "                    nn.Linear(input_dim, hidden_dim) for _ in range(3)\n",
        "                ])\n",
        "\n",
        "                # Attention\n",
        "                self.attention = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "                # 출력\n",
        "                self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "            def forward(self, x_list, adj_list):\n",
        "                # x_list: 시간 단계별 피쳐 리스트\n",
        "                # adj_list: 시간 단계별 인접 행렬 리스트\n",
        "\n",
        "                temporal_embeddings = []\n",
        "\n",
        "                for t, (x, adj) in enumerate(zip(x_list, adj_list)):\n",
        "                    if t < len(self.temporal_gcn):\n",
        "                        h = torch.mm(adj, x)\n",
        "                        h = F.relu(self.temporal_gcn[t](h))\n",
        "                        temporal_embeddings.append(h)\n",
        "\n",
        "                # Temporal attention\n",
        "                if len(temporal_embeddings) > 0:\n",
        "                    embeddings_stacked = torch.stack(temporal_embeddings, dim=1)\n",
        "                    attention_weights = F.softmax(\n",
        "                        self.attention(embeddings_stacked).squeeze(-1),\n",
        "                        dim=1\n",
        "                    )\n",
        "                    h_final = torch.sum(\n",
        "                        embeddings_stacked * attention_weights.unsqueeze(-1),\n",
        "                        dim=1\n",
        "                    )\n",
        "                else:\n",
        "                    h_final = x_list[0]\n",
        "\n",
        "                output = torch.sigmoid(self.fc(h_final))\n",
        "                return output\n",
        "\n",
        "        return TemporalGNN(self.input_dim, self.hidden_dim)\n",
        "\n",
        "    def build_dual_view_hybrid(self):\n",
        "        \"\"\"Dual-View Hybrid (구조적 + 행위적 뷰)\"\"\"\n",
        "\n",
        "        class DualViewHybrid(nn.Module):\n",
        "            def __init__(self, input_dim, hidden_dim):\n",
        "                super(DualViewHybrid, self).__init__()\n",
        "\n",
        "                # 구조적 뷰 (그래프 중심성 등)\n",
        "                self.structural_encoder = nn.Sequential(\n",
        "                    nn.Linear(input_dim // 2, hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3)\n",
        "                )\n",
        "\n",
        "                # 행위적 뷰 (거래 패턴 등)\n",
        "                self.behavioral_encoder = nn.Sequential(\n",
        "                    nn.Linear(input_dim // 2, hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.3)\n",
        "                )\n",
        "\n",
        "                # Contrastive learning용 projection\n",
        "                self.projection = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "                # 분류기\n",
        "                self.classifier = nn.Sequential(\n",
        "                    nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(hidden_dim, 1),\n",
        "                    nn.Sigmoid()\n",
        "                )\n",
        "\n",
        "            def forward(self, x_structural, x_behavioral):\n",
        "                # 각 뷰별 인코딩\n",
        "                h_struct = self.structural_encoder(x_structural)\n",
        "                h_behav = self.behavioral_encoder(x_behavioral)\n",
        "\n",
        "                # Projection (대조 학습)\n",
        "                z_struct = self.projection(h_struct)\n",
        "                z_behav = self.projection(h_behav)\n",
        "\n",
        "                # 융합 및 분류\n",
        "                h_combined = torch.cat([h_struct, h_behav], dim=1)\n",
        "                output = self.classifier(h_combined)\n",
        "\n",
        "                return output, z_struct, z_behav\n",
        "\n",
        "        return DualViewHybrid(self.input_dim, self.hidden_dim)\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 5. 백테스트 및 시뮬레이션\n",
        "# ====================================\n",
        "\n",
        "class AMLBacktester:\n",
        "    \"\"\"과거 데이터 기반 백테스트 및 시뮬레이션\"\"\"\n",
        "\n",
        "    def __init__(self, model, threshold=0.5):\n",
        "        self.model = model\n",
        "        self.threshold = threshold\n",
        "        self.simulation_results = []\n",
        "\n",
        "    def run_backtest(self, X_test, y_test, test_df, transaction_data):\n",
        "        \"\"\"백테스트 실행\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"백테스트 시뮬레이션\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # 예측\n",
        "        y_pred_proba = self.model.predict_proba(X_test)[:, 1]\n",
        "        y_pred = (y_pred_proba >= self.threshold).astype(int)\n",
        "\n",
        "        # 시뮬레이션 결과 저장\n",
        "        results_df = test_df.copy()\n",
        "        results_df['predicted_proba'] = y_pred_proba\n",
        "        results_df['predicted'] = y_pred\n",
        "        results_df['actual'] = y_test.values\n",
        "\n",
        "        # 탐지된 케이스 분석\n",
        "        detected_positive = results_df[\n",
        "            (results_df['predicted'] == 1) &\n",
        "            (results_df['actual'] == 1)\n",
        "        ]\n",
        "\n",
        "        false_positive = results_df[\n",
        "            (results_df['predicted'] == 1) &\n",
        "            (results_df['actual'] == 0)\n",
        "        ]\n",
        "\n",
        "        missed_cases = results_df[\n",
        "            (results_df['predicted'] == 0) &\n",
        "            (results_df['actual'] == 1)\n",
        "        ]\n",
        "\n",
        "        # 금액 계산\n",
        "        total_laundering_amount = self._calculate_total_amount(\n",
        "            detected_positive, transaction_data\n",
        "        )\n",
        "        prevented_amount = total_laundering_amount\n",
        "        missed_amount = self._calculate_total_amount(\n",
        "            missed_cases, transaction_data\n",
        "        )\n",
        "\n",
        "        # 결과 출력\n",
        "        print(f\"\\n[시뮬레이션 기간]\")\n",
        "        print(f\"  시작: {results_df['TimeUnit'].min()}\")\n",
        "        print(f\"  종료: {results_df['TimeUnit'].max()}\")\n",
        "\n",
        "        print(f\"\\n[탐지 결과]\")\n",
        "        print(f\"  정탐 (TP): {len(detected_positive)}건\")\n",
        "        print(f\"  오탐 (FP): {len(false_positive)}건\")\n",
        "        print(f\"  미탐 (FN): {len(missed_cases)}건\")\n",
        "\n",
        "        print(f\"\\n[금액 분석]\")\n",
        "        print(f\"  탐지한 자금세탁 금액: ${prevented_amount:,.2f}\")\n",
        "        print(f\"  놓친 자금세탁 금액: ${missed_amount:,.2f}\")\n",
        "        print(f\"  탐지율 (금액 기준): {prevented_amount/(prevented_amount+missed_amount)*100:.2f}%\")\n",
        "\n",
        "        # 시계열 분석\n",
        "        self._plot_temporal_detection(results_df)\n",
        "\n",
        "        # 상세 케이스 분석\n",
        "        self._analyze_detection_cases(detected_positive, false_positive, missed_cases)\n",
        "\n",
        "        return results_df\n",
        "\n",
        "    def _calculate_total_amount(self, cases_df, transaction_data):\n",
        "        \"\"\"케이스들의 총 거래 금액 계산\"\"\"\n",
        "        if len(cases_df) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        total = 0.0\n",
        "        for _, case in cases_df.iterrows():\n",
        "            account = case['Account']\n",
        "            time_unit = case['TimeUnit']\n",
        "\n",
        "            # 해당 계좌의 해당 시간대 거래 금액 합산\n",
        "            relevant_trans = transaction_data[\n",
        "                (transaction_data['Account'] == account) &\n",
        "                (transaction_data['TimeUnit'] == time_unit)\n",
        "            ]\n",
        "\n",
        "            if len(relevant_trans) > 0:\n",
        "                total += relevant_trans['Amount Received'].sum()\n",
        "\n",
        "        return total\n",
        "\n",
        "    def _plot_temporal_detection(self, results_df):\n",
        "        \"\"\"시계열 탐지 결과 시각화\"\"\"\n",
        "        print(\"\\n[시계열 탐지 패턴]\")\n",
        "\n",
        "        # 시간대별 탐지 현황\n",
        "        time_detection = results_df.groupby('TimeUnit').agg({\n",
        "            'actual': 'sum',\n",
        "            'predicted': 'sum'\n",
        "        }).reset_index()\n",
        "\n",
        "        time_detection.columns = ['TimeUnit', 'Actual_Laundering', 'Detected']\n",
        "\n",
        "        print(f\"\\n시간대별 탐지 현황 (샘플):\")\n",
        "        print(time_detection.head(10))\n",
        "\n",
        "    def _analyze_detection_cases(self, detected, false_pos, missed):\n",
        "        \"\"\"탐지 케이스 상세 분석\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"케이스별 상세 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        if len(detected) > 0:\n",
        "            print(f\"\\n✅ 성공 탐지 케이스 (샘플 5건):\")\n",
        "            print(detected[['Account', 'TimeUnit', 'predicted_proba']].head())\n",
        "\n",
        "        if len(false_pos) > 0:\n",
        "            print(f\"\\n⚠️ 오탐 케이스 (샘플 5건):\")\n",
        "            print(false_pos[['Account', 'TimeUnit', 'predicted_proba']].head())\n",
        "\n",
        "        if len(missed) > 0:\n",
        "            print(f\"\\n❌ 미탐 케이스 (샘플 5건):\")\n",
        "            print(missed[['Account', 'TimeUnit', 'predicted_proba']].head())\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 6. XAI - SHAP 및 GNN 설명\n",
        "# ====================================\n",
        "\n",
        "class ModelExplainer:\n",
        "    \"\"\"모델 설명 가능성 분석\"\"\"\n",
        "\n",
        "    def __init__(self, model, feature_names):\n",
        "        self.model = model\n",
        "        self.feature_names = feature_names\n",
        "\n",
        "    def explain_with_shap(self, X_train, X_test):\n",
        "        \"\"\"SHAP 기반 피쳐 중요도\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"SHAP 피쳐 중요도 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        try:\n",
        "            import shap\n",
        "\n",
        "            # Tree explainer\n",
        "            explainer = shap.TreeExplainer(self.model)\n",
        "            shap_values = explainer.shap_values(X_test.iloc[:100])\n",
        "\n",
        "            # 피쳐 중요도\n",
        "            feature_importance = pd.DataFrame({\n",
        "                'feature': self.feature_names,\n",
        "                'importance': np.abs(shap_values).mean(axis=0)\n",
        "            }).sort_values('importance', ascending=False)\n",
        "\n",
        "            print(\"\\n상위 20개 중요 피쳐:\")\n",
        "            print(feature_importance.head(20))\n",
        "\n",
        "            # 그래프 피쳐 vs 비그래프 피쳐 비교\n",
        "            graph_keywords = ['centrality', 'degree', 'pagerank', 'betweenness',\n",
        "                            'successors', 'predecessors', 'weight']\n",
        "\n",
        "            feature_importance['is_graph'] = feature_importance['feature'].apply(\n",
        "                lambda x: any(kw in x.lower() for kw in graph_keywords)\n",
        "            )\n",
        "\n",
        "            graph_importance = feature_importance[feature_importance['is_graph']]['importance'].sum()\n",
        "            non_graph_importance = feature_importance[~feature_importance['is_graph']]['importance'].sum()\n",
        "\n",
        "            print(f\"\\n그래프 피쳐 총 중요도: {graph_importance:.4f}\")\n",
        "            print(f\"비그래프 피쳐 총 중요도: {non_graph_importance:.4f}\")\n",
        "            print(f\"그래프 피쳐 기여도: {graph_importance/(graph_importance+non_graph_importance)*100:.2f}%\")\n",
        "\n",
        "            return feature_importance\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"⚠️ SHAP 라이브러리가 설치되지 않았습니다.\")\n",
        "            return None\n",
        "\n",
        "    def explain_gnn_attention(self, gnn_model, node_features, adj_matrix):\n",
        "        \"\"\"GNN attention 가중치 분석\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"GNN Attention 분석\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Attention 가중치 추출 (모델에 attention이 있는 경우)\n",
        "        try:\n",
        "            # 간단한 예시: 각 노드의 이웃에 대한 attention\n",
        "            # 실제로는 모델 구조에 따라 다름\n",
        "\n",
        "            attention_weights = torch.softmax(\n",
        "                torch.mm(node_features, node_features.t()),\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # 각 노드별 주요 이웃 분석\n",
        "            top_k = 5\n",
        "            for node_idx in range(min(10, len(node_features))):\n",
        "                top_neighbors = torch.topk(attention_weights[node_idx], top_k)\n",
        "                print(f\"\\n노드 {node_idx}의 주요 이웃 (Top {top_k}):\")\n",
        "                for i, (weight, neighbor) in enumerate(zip(top_neighbors.values, top_neighbors.indices)):\n",
        "                    print(f\"  {i+1}. 노드 {neighbor.item()}: {weight.item():.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ GNN Attention 분석 실패: {e}\")\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 7. 통합 실험 프레임워크\n",
        "# ====================================\n",
        "\n",
        "class ExperimentFramework:\n",
        "    \"\"\"전체 실험 관리 및 비교\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.experiments = {}\n",
        "        self.results_summary = []\n",
        "\n",
        "    def run_experiment(self, exp_name, train_fn, X_train, y_train, X_test, y_test):\n",
        "        \"\"\"실험 실행\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"실험: {exp_name}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        # 학습 및 예측\n",
        "        model, y_pred_proba = train_fn(X_train, y_train, X_test, y_test)\n",
        "\n",
        "        # 평가\n",
        "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "        metrics = {\n",
        "            'experiment': exp_name,\n",
        "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
        "            'auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0\n",
        "        }\n",
        "\n",
        "        self.experiments[exp_name] = {\n",
        "            'model': model,\n",
        "            'predictions': y_pred_proba,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "\n",
        "        self.results_summary.append(metrics)\n",
        "\n",
        "        print(f\"\\n[결과]\")\n",
        "        for metric, value in metrics.items():\n",
        "            if metric != 'experiment':\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "        return model, y_pred_proba\n",
        "\n",
        "    def compare_all_experiments(self):\n",
        "        \"\"\"모든 실험 결과 비교\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"전체 실험 결과 비교\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        df_results = pd.DataFrame(self.results_summary)\n",
        "        df_results = df_results.set_index('experiment')\n",
        "\n",
        "        print(\"\\n\", df_results)\n",
        "\n",
        "        # 최고 성능 모델\n",
        "        best_f1_exp = df_results['f1'].idxmax()\n",
        "        print(f\"\\n🏆 최고 F1 Score: {best_f1_exp} ({df_results.loc[best_f1_exp, 'f1']:.4f})\")\n",
        "\n",
        "        # 시각화\n",
        "        self._plot_comparison(df_results)\n",
        "\n",
        "        return df_results\n",
        "\n",
        "    def _plot_comparison(self, df_results):\n",
        "        \"\"\"결과 비교 시각화\"\"\"\n",
        "        print(\"\\n[성능 비교 차트]\")\n",
        "        print(\"(실제 환경에서는 matplotlib으로 시각화)\")\n",
        "\n",
        "        # 각 지표별 비교\n",
        "        for metric in ['precision', 'recall', 'f1', 'auc']:\n",
        "            print(f\"\\n{metric.upper()}:\")\n",
        "            for exp in df_results.index:\n",
        "                value = df_results.loc[exp, metric]\n",
        "                bar = '█' * int(value * 50)\n",
        "                print(f\"  {exp:30s} {bar} {value:.4f}\")\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 8. 메인 실행 함수\n",
        "# ====================================\n",
        "\n",
        "def run_all_experiments(X_train, y_train, X_test, y_test,\n",
        "                       X_train_no_graph, X_test_no_graph,\n",
        "                       baseline_model, transaction_data, test_df):\n",
        "    \"\"\"전체 실험 실행\"\"\"\n",
        "\n",
        "    framework = ExperimentFramework()\n",
        "\n",
        "    # ========== 실험 1: Baseline (그래프 피쳐 없음) ==========\n",
        "    def exp1_baseline(X_tr, y_tr, X_te, y_te):\n",
        "        from xgboost import XGBClassifier\n",
        "        model = XGBClassifier(random_state=42, scale_pos_weight=10)\n",
        "        model.fit(X_tr, y_tr)\n",
        "        pred = model.predict_proba(X_te)[:, 1]\n",
        "        return model, pred\n",
        "\n",
        "    framework.run_experiment(\n",
        "        \"1. Baseline (No Graph Features)\",\n",
        "        exp1_baseline,\n",
        "        X_train_no_graph, y_train,\n",
        "        X_test_no_graph, y_test\n",
        "    )\n",
        "\n",
        "    # ========== 실험 2: 그래프 피쳐 추가 ==========\n",
        "    def exp2_with_graph(X_tr, y_tr, X_te, y_te):\n",
        "        from xgboost import XGBClassifier\n",
        "        model = XGBClassifier(random_state=42, scale_pos_weight=10)\n",
        "        model.fit(X_tr, y_tr)\n",
        "        pred = model.predict_proba(X_te)[:, 1]\n",
        "        return model, pred\n",
        "\n",
        "    framework.run_experiment(\n",
        "        \"2. With Graph Features\",\n",
        "        exp2_with_graph,\n",
        "        X_train, y_train,\n",
        "        X_test, y_test\n",
        "    )\n",
        "\n",
        "    # ========== 실험 3: 시계열 앙상블 ==========\n",
        "    print(\"\\n⚠️ 시계열 앙상블 실험은 충분한 시계열 데이터가 필요합니다.\")\n",
        "    # 실제 구현 시 활성화\n",
        "\n",
        "    # ========== 실험 4: 간소화 GNN ==========\n",
        "    print(\"\\n⚠️ GNN 실험은 그래프 구조 데이터가 필요합니다.\")\n",
        "    # 실제 구현 시 활성화\n",
        "\n",
        "    # ========== 실험 결과 비교 ==========\n",
        "    results_df = framework.compare_all_experiments()\n",
        "\n",
        "    # ========== 백테스트 ==========\n",
        "    best_model = framework.experiments[\"2. With Graph Features\"]['model']\n",
        "    backtester = AMLBacktester(best_model)\n",
        "    backtest_results = backtester.run_backtest(\n",
        "        X_test, y_test, test_df, transaction_data\n",
        "    )\n",
        "\n",
        "    # ========== XAI 분석 ==========\n",
        "    explainer = ModelExplainer(best_model, X_train.columns.tolist())\n",
        "    feature_importance = explainer.explain_with_shap(X_train, X_test)\n",
        "\n",
        "    return framework, backtest_results, feature_importance\n",
        "\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"AML 고도화 모델 및 실험 프레임워크 로드 완료\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "\n",
        "# ====================================\n",
        "# 8. 고도화 실험 실행 파이프라인\n",
        "# ====================================\n",
        "\n",
        "def main_with_experiments():\n",
        "    \"\"\"전체 파이프라인 + 고도화 실험 실행\"\"\"\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"금융결제원 AML 탐지 프로젝트 - 고도화 실험\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # ========== 1. 기본 데이터 로드 및 전처리 ==========\n",
        "    loader = AMLDataLoader(\n",
        "        trans_path=trans_path,\n",
        "        accounts_path=accounts_path\n",
        "    )\n",
        "\n",
        "    df_trans, df_accounts = loader.load_data()\n",
        "    df_trans = loader.explore_data(df_trans, df_accounts)\n",
        "\n",
        "    # 샘플링 비율을 높여서 더 많은 데이터 확보\n",
        "    preprocessor = AMLPreprocessor(sample_ratio=0.3)    # 10% → 30%로 증가\n",
        "\n",
        "    # 시간 단위 배치 샘플 생성\n",
        "    all_trans, account_time_labels = preprocessor.create_hourly_samples(df_trans)\n",
        "    sampled_labels = preprocessor.stratified_sample(account_time_labels)\n",
        "\n",
        "    print(f\"\\n전체 계좌-시간 샘플: {len(account_time_labels)}건\")\n",
        "    print(f\"자금세탁 샘플: {account_time_labels['Is Laundering'].sum()}건\")\n",
        "\n",
        "    # 자금세탁 패턴 분석\n",
        "    laundering, normal = preprocessor.analyze_laundering_patterns(df_trans)\n",
        "\n",
        "    # 샘플링 (큰 데이터를 작게)\n",
        "    sampled_labels = preprocessor.stratified_sample(account_time_labels)\n",
        "\n",
        "    print(f\"\\n샘플링 후:\")\n",
        "    print(f\"  총 샘플: {len(sampled_labels)}건\")\n",
        "    print(f\"  자금세탁: {sampled_labels['Is Laundering'].sum()}건\")\n",
        "    print(f\"  자금세탁 비율: {sampled_labels['Is Laundering'].mean():.4%}\")\n",
        "\n",
        "    # 자금세탁 건이 너무 적으면 경고\n",
        "    if sampled_labels['Is Laundering'].sum() < 10:\n",
        "        print(\"\\n⚠️ 경고: 자금세탁 샘플이 너무 적습니다!\")\n",
        "        print(\"sample_ratio를 더 높이거나 전체 데이터를 사용하세요.\")\n",
        "        print(\"일단 전체 데이터로 진행합니다...\")\n",
        "        sampled_labels = account_time_labels\n",
        "\n",
        "    # ========== 2. 피쳐 생성 ==========\n",
        "    feature_engineer = FeatureEngineer()\n",
        "\n",
        "    # 집계 피쳐 생성 (더 많은 샘플 처리)\n",
        "    print(\"\\n⚠️ 주의: 피쳐 생성에 시간이 걸릴 수 있습니다.\")\n",
        "    print(f\"최대 {min(2000, len(sampled_labels))}건 처리\")\n",
        "\n",
        "    # 집계 피쳐 (그래프 피쳐 제외)\n",
        "    agg_features = feature_engineer.create_aggregation_features(\n",
        "        all_trans,\n",
        "        sampled_labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\n집계 피쳐 생성 완료: {len(agg_features)}건\")\n",
        "\n",
        "    # 그래프 피쳐 생성\n",
        "    graph_features = feature_engineer.create_graph_features(\n",
        "        all_trans,\n",
        "        sampled_labels.head(len(agg_features))    # 집계 피쳐와 동일한 수만큼\n",
        "    )\n",
        "\n",
        "    print(f\"그래프 피쳐 생성 완료: {len(graph_features)}건\")\n",
        "\n",
        "    # 그래프 피쳐명 추출\n",
        "    graph_feature_names = [c for c in graph_features.columns\n",
        "                          if c not in ['Account', 'TimeUnit']]\n",
        "\n",
        "    print(f\"\\n그래프 피쳐 수: {len(graph_feature_names)}개\")\n",
        "    print(f\"그래프 피쳐: {graph_feature_names}\")\n",
        "\n",
        "    # 피쳐 병합 (그래프 포함)\n",
        "    all_features = agg_features.merge(\n",
        "        graph_features,\n",
        "        on=['Account', 'TimeUnit'],\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    print(f\"\\n피쳐 병합 후: {len(all_features)}건\")\n",
        "    print(f\"전체 피쳐 수: {len([c for c in all_features.columns if c not in ['Account', 'TimeUnit']])}\")\n",
        "\n",
        "    # 그래프 피쳐 제외 버전\n",
        "    features_no_graph = agg_features.copy()\n",
        "\n",
        "    # ========== 3.모델 학습 ==========\n",
        "    trainer = AMLModelTrainer()\n",
        "\n",
        "    # Train/Test 분할 (test_size를 크게 해서 테스트 데이터 확보)\n",
        "    # 그래프 피쳐 포함 버전\n",
        "    X_train, X_test, y_train, y_test, test_df = trainer.prepare_train_test_split(\n",
        "        all_features,\n",
        "        sampled_labels,\n",
        "        test_size=0.4   # 30% → 40%로 증가\n",
        "    )\n",
        "    # 그래프 피쳐 제외 버전\n",
        "    X_train_no_graph, X_test_no_graph, _, _, _ = trainer.prepare_train_test_split(\n",
        "        features_no_graph,\n",
        "        sampled_labels,\n",
        "        test_size=0.4\n",
        "    )\n",
        "\n",
        "    # 데이터가 없으면 중단\n",
        "    if len(X_train) == 0 or len(X_test) == 0:\n",
        "        print(\"\\n⚠️ 학습 또는 테스트 데이터가 없습니다. 프로세스를 중단합니다.\")\n",
        "        print(\"\\n문제 해결 방법:\")\n",
        "        print(\"1. sample_ratio를 1.0으로 설정 (전체 데이터 사용)\")\n",
        "        print(\"2. 피쳐 생성의 max_samples 제한 제거\")\n",
        "        print(\"3. 데이터 파일 경로 확인\")\n",
        "        return None\n",
        "\n",
        "    # Baseline 모델 학습\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Baseline 모델 학습\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    baseline_model, baseline_pred = trainer.train_baseline_model(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        use_smote=False,  # SMOTE 사용 여부\n",
        "        scale_pos_weight=None  # Auto 계산\n",
        "    )\n",
        "\n",
        "    # ========== 5. 평가 ==========\n",
        "    # Top-K 평가\n",
        "    topk_results = trainer.evaluate_topk(\n",
        "        y_test.values,\n",
        "        y_pred_proba,\n",
        "        test_df,\n",
        "        k_values=[10, 20, 50]  # K 값을 작게 조정\n",
        "    )\n",
        "\n",
        "    # Feature Importance (XAI)\n",
        "    if len(X_train) > 0 and len(X_test) > 0:\n",
        "        feature_names = [c for c in X_train.columns]\n",
        "        feature_importance = trainer.explain_with_shap(\n",
        "            baseline_model,\n",
        "            X_train,\n",
        "            X_test,\n",
        "            feature_names\n",
        "        )\n",
        "    else:\n",
        "        feature_importance = None\n",
        "\n",
        "    # ========== 5. 고도화 실험 실행 ==========\n",
        "\n",
        "    # 실험 프레임워크 초기화\n",
        "    exp_framework = ExperimentFramework()\n",
        "\n",
        "    # ---------- 실험 1: 그래프 피쳐 없음 ----------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"실험 1: Baseline (그래프 피쳐 제외)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    def train_no_graph(X_tr, y_tr, X_te, y_te):\n",
        "        scale_pos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum() if y_tr.sum() > 0 else 1.0\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        pred = model.predict_proba(X_te)[:, 1]\n",
        "        return model, pred\n",
        "\n",
        "    model_no_graph, pred_no_graph = exp_framework.run_experiment(\n",
        "        \"Baseline (No Graph)\",\n",
        "        train_no_graph,\n",
        "        X_train_no_graph, y_train,\n",
        "        X_test_no_graph, y_test\n",
        "    )\n",
        "\n",
        "    # ---------- 실험 2: 그래프 피쳐 포함 ----------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"실험 2: 그래프 피쳐 포함\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    def train_with_graph(X_tr, y_tr, X_te, y_te):\n",
        "        scale_pos_weight = (len(y_tr) - y_tr.sum()) / y_tr.sum() if y_tr.sum() > 0 else 1.0\n",
        "        model = XGBClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            scale_pos_weight=scale_pos_weight,\n",
        "            random_state=42,\n",
        "            eval_metric='logloss'\n",
        "        )\n",
        "        model.fit(X_tr, y_tr)\n",
        "        pred = model.predict_proba(X_te)[:, 1]\n",
        "        return model, pred\n",
        "\n",
        "    model_with_graph, pred_with_graph = exp_framework.run_experiment(\n",
        "        \"With Graph Features\",\n",
        "        train_with_graph,\n",
        "        X_train, y_train,\n",
        "        X_test, y_test\n",
        "    )\n",
        "\n",
        "    # ---------- 실험 3: 시계열 앙상블 ----------\n",
        "    if len(X_train) >= 10:  # 최소 데이터 요구사항\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"실험 3: 시계열 앙상블\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        try:\n",
        "            ensemble = TimeSeriesEnsemble(model_with_graph)\n",
        "            ensemble.train_lstm(X_train, y_train, epochs=10)\n",
        "            pred_ensemble = ensemble.predict_ensemble(X_test)\n",
        "\n",
        "            # 수동으로 메트릭 계산\n",
        "            y_pred_ensemble = (pred_ensemble >= 0.5).astype(int)\n",
        "            from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "            metrics_ensemble = {\n",
        "                'experiment': 'Time Series Ensemble',\n",
        "                'precision': precision_score(y_test, y_pred_ensemble, zero_division=0),\n",
        "                'recall': recall_score(y_test, y_pred_ensemble, zero_division=0),\n",
        "                'f1': f1_score(y_test, y_pred_ensemble, zero_division=0),\n",
        "                'auc': roc_auc_score(y_test, pred_ensemble) if len(np.unique(y_test)) > 1 else 0\n",
        "            }\n",
        "\n",
        "            exp_framework.results_summary.append(metrics_ensemble)\n",
        "\n",
        "            print(f\"\\n[시계열 앙상블 결과]\")\n",
        "            for metric, value in metrics_ensemble.items():\n",
        "                if metric != 'experiment':\n",
        "                    print(f\"  {metric}: {value:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ 시계열 앙상블 실패: {e}\")\n",
        "    else:\n",
        "        print(\"\\n⚠️ 시계열 앙상블: 데이터 부족으로 스킵\")\n",
        "\n",
        "    # ---------- 실험 4: 간소화 GNN ----------\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"실험 4: 간소화 GNN\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"⚠️ GNN 실험은 에지 리스트 생성이 필요합니다.\")\n",
        "    print(\"현재는 스킵하고, 필요 시 활성화할 수 있습니다.\")\n",
        "\n",
        "    # GNN 실험 예시 (실제 구현 시)\n",
        "    # from aml_advanced_models import SimpleGNN\n",
        "    # gnn = SimpleGNN(input_dim=X_train.shape[1])\n",
        "    # edge_list = [(0, 1), (1, 2), ...]  # 거래 그래프 엣지\n",
        "    # gnn.train_model(X_train.values, y_train, edge_list)\n",
        "    # pred_gnn = gnn.predict(X_test.values, edge_list)\n",
        "\n",
        "    # ========== 6. 전체 실험 결과 비교 ==========\n",
        "    results_df = exp_framework.compare_all_experiments()\n",
        "\n",
        "    # ========== 7. 백테스트 시뮬레이션 ==========\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"백테스트 시뮬레이션\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    best_model = model_with_graph\n",
        "    backtester = AMLBacktester(best_model, threshold=0.5)\n",
        "\n",
        "    backtest_results = backtester.run_backtest(\n",
        "        X_test, y_test, test_df, all_trans\n",
        "    )\n",
        "\n",
        "    # ========== 8. XAI 분석 ==========\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"XAI - 모델 설명 가능성 분석\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    explainer = ModelExplainer(best_model, X_train.columns.tolist())\n",
        "    feature_importance = explainer.explain_with_shap(X_train, X_test)\n",
        "\n",
        "    # ========== 9. 최종 요약 ==========\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"최종 요약\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(\"\\n[실험 결과 요약]\")\n",
        "    print(results_df)\n",
        "\n",
        "    if feature_importance is not None:\n",
        "        print(\"\\n[Top 10 중요 피쳐]\")\n",
        "        print(feature_importance.head(10))\n",
        "\n",
        "    print(\"\\n[권장 사항]\")\n",
        "    best_exp = results_df['f1'].idxmax()\n",
        "    print(f\"  1. 최고 성능 모델: {best_exp}\")\n",
        "    print(f\"     - F1 Score: {results_df.loc[best_exp, 'f1']:.4f}\")\n",
        "    print(f\"  2. 그래프 피쳐의 효과:\")\n",
        "\n",
        "    if 'With Graph Features' in results_df.index and 'Baseline (No Graph)' in results_df.index:\n",
        "        f1_with = results_df.loc['With Graph Features', 'f1']\n",
        "        f1_without = results_df.loc['Baseline (No Graph)', 'f1']\n",
        "        improvement = ((f1_with - f1_without) / f1_without * 100) if f1_without > 0 else 0\n",
        "        print(f\"     - F1 Score 개선: {improvement:+.2f}%\")\n",
        "\n",
        "    print(f\"  3. 백테스트 결과:\")\n",
        "    if backtest_results is not None and len(backtest_results) > 0:\n",
        "        detected_count = len(backtest_results[\n",
        "            (backtest_results['predicted'] == 1) &\n",
        "            (backtest_results['actual'] == 1)\n",
        "        ])\n",
        "        total_laundering = backtest_results['actual'].sum()\n",
        "        print(f\"     - 탐지율: {detected_count}/{total_laundering} \"\n",
        "              f\"({detected_count/total_laundering*100:.2f}%)\" if total_laundering > 0 else \"N/A\")\n",
        "\n",
        "    return {\n",
        "        'experiment_framework': exp_framework,\n",
        "        'results': results_df,\n",
        "        'backtest': backtest_results,\n",
        "        'feature_importance': feature_importance,\n",
        "        'best_model': best_model\n",
        "    }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7OmeFb3v1cf",
        "outputId": "16e3a6b7-03fd-4a4e-d646-4c0eba1c8e90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "AML 고도화 모델 및 실험 프레임워크 로드 완료\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 실행\n",
        "    results = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dZpkBwmlDaxU",
        "outputId": "9dde9ba5-6d86-458f-8732-3e8ef9881e1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "금융결제원 AML 탐지 프로젝트 시작\n",
            "================================================================================\n",
            "================================================================================\n",
            "데이터 로딩 중...\n",
            "거래 데이터 shape: (5078345, 11)\n",
            "계좌 데이터 shape: (518581, 5)\n",
            "\n",
            "================================================================================\n",
            "데이터 탐색\n",
            "================================================================================\n",
            "\n",
            "[거래 데이터 샘플]\n",
            "          Timestamp  From Bank    Account  To Bank  Account.1  \\\n",
            "0  2022/09/01 00:20         10  8000EBD30       10  8000EBD30   \n",
            "1  2022/09/01 00:20       3208  8000F4580        1  8000F5340   \n",
            "2  2022/09/01 00:00       3209  8000F4670     3209  8000F4670   \n",
            "3  2022/09/01 00:02         12  8000F5030       12  8000F5030   \n",
            "4  2022/09/01 00:06         10  8000F5200       10  8000F5200   \n",
            "\n",
            "   Amount Received Receiving Currency  Amount Paid Payment Currency  \\\n",
            "0          3697.34          US Dollar      3697.34        US Dollar   \n",
            "1             0.01          US Dollar         0.01        US Dollar   \n",
            "2         14675.57          US Dollar     14675.57        US Dollar   \n",
            "3          2806.97          US Dollar      2806.97        US Dollar   \n",
            "4         36682.97          US Dollar     36682.97        US Dollar   \n",
            "\n",
            "  Payment Format  Is Laundering  \n",
            "0   Reinvestment              0  \n",
            "1         Cheque              0  \n",
            "2   Reinvestment              0  \n",
            "3   Reinvestment              0  \n",
            "4   Reinvestment              0  \n",
            "\n",
            "컬럼: ['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1', 'Amount Received', 'Receiving Currency', 'Amount Paid', 'Payment Currency', 'Payment Format', 'Is Laundering']\n",
            "\n",
            "결측치:\n",
            "Timestamp             0\n",
            "From Bank             0\n",
            "Account               0\n",
            "To Bank               0\n",
            "Account.1             0\n",
            "Amount Received       0\n",
            "Receiving Currency    0\n",
            "Amount Paid           0\n",
            "Payment Currency      0\n",
            "Payment Format        0\n",
            "Is Laundering         0\n",
            "dtype: int64\n",
            "\n",
            "[자금세탁 분포]\n",
            "Is Laundering\n",
            "0    5073168\n",
            "1       5177\n",
            "Name: count, dtype: int64\n",
            "자금세탁 비율: 0.1019%\n",
            "\n",
            "[시간 범위]\n",
            "시작: 2022-09-01 00:00:00\n",
            "종료: 2022-09-18 16:18:00\n",
            "\n",
            "[일별 자금세탁 분포]\n",
            "     sum    count      mean\n",
            "Day                        \n",
            "1    322  1114921  0.000289\n",
            "2    408   754449  0.000541\n",
            "3    391   207382  0.001885\n",
            "4    407   207430  0.001962\n",
            "5    471   482650  0.000976\n",
            "6    531   482089  0.001101\n",
            "7    497   482751  0.001030\n",
            "8    539   482773  0.001116\n",
            "9    514   654467  0.000785\n",
            "10   442   208325  0.002122\n",
            "11   232      396  0.585859\n",
            "12   170      281  0.604982\n",
            "13   106      184  0.576087\n",
            "14    70      121  0.578512\n",
            "15    28       46  0.608696\n",
            "16    26       46  0.565217\n",
            "17    15       23  0.652174\n",
            "18     8       11  0.727273\n",
            "\n",
            "⚠️ Day 피쳐의 label 분포가 치우쳐져 있습니다. 제거 고려 필요\n",
            "\n",
            "[계좌 데이터 샘플]\n",
            "                     Bank Name  Bank ID Account Number  Entity ID  \\\n",
            "0          Portugal Bank #4507   331579      80B779D80  80062E240   \n",
            "1              Canada Bank #27      210      809D86900  800C998A0   \n",
            "2                  UK Bank #33    21884      80812BE00  800C47F50   \n",
            "3           Germany Bank #4815    32742      81047F300  80096F0B0   \n",
            "4  National Bank of Harrisburg   127390      80BD8CF00  800FB8760   \n",
            "\n",
            "                  Entity Name  \n",
            "0  Sole Proprietorship #50438  \n",
            "1          Corporation #33520  \n",
            "2          Partnership #35397  \n",
            "3          Corporation #48813  \n",
            "4            Corporation #889  \n",
            "\n",
            "은행 분포:\n",
            "Bank Name\n",
            "National Bank of Laramie       3797\n",
            "National Bank of the East      3663\n",
            "Japan Bank #0                  3051\n",
            "Arbor Savings Bank             2894\n",
            "National Bank of Pittsburgh    2683\n",
            "Savings Bank of Fairfield      2642\n",
            "First Bank of Tampa            2464\n",
            "Savings Bank of Huron          2379\n",
            "Savings Bank of Los Angeles    2052\n",
            "Golden Bancorp                 1967\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "시간 단위 배치 샘플 생성\n",
            "================================================================================\n",
            "총 샘플 수 (계좌-시간 단위): 4586703\n",
            "자금세탁 샘플: 10058\n",
            "\n",
            "계층화 샘플링 (비율: 0.3)\n",
            "샘플링 후 크기: 1376010\n",
            "자금세탁 비율: 0.2193%\n",
            "\n",
            "전체 계좌-시간 샘플: 4586703건\n",
            "자금세탁 샘플: 10058건\n",
            "\n",
            "================================================================================\n",
            "자금세탁 패턴 분석\n",
            "================================================================================\n",
            "\n",
            "[거래 금액 통계]\n",
            "자금세탁 - 평균: $36135310.41, 중앙값: $8667.21\n",
            "정상 거래 - 평균: $5957962.48, 중앙값: $1407.51\n",
            "\n",
            "[결제 수단 분포]\n",
            "자금세탁:\n",
            "Receiving Currency\n",
            "US Dollar      1912\n",
            "Euro           1372\n",
            "Saudi Riyal     374\n",
            "Swiss Franc     193\n",
            "Yuan            184\n",
            "Name: count, dtype: int64\n",
            "\n",
            "정상 거래:\n",
            "Receiving Currency\n",
            "US Dollar      1877429\n",
            "Euro           1170645\n",
            "Swiss Franc     237691\n",
            "Yuan            206367\n",
            "Shekel          194893\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[시간대 분포]\n",
            "자금세탁 - 시간대별:\n",
            "Hour\n",
            "0     176\n",
            "1     152\n",
            "2     165\n",
            "3     146\n",
            "4     154\n",
            "5     188\n",
            "6     207\n",
            "7     195\n",
            "8     258\n",
            "9     217\n",
            "10    234\n",
            "11    295\n",
            "12    336\n",
            "13    292\n",
            "14    279\n",
            "15    263\n",
            "16    311\n",
            "17    257\n",
            "18    255\n",
            "19    231\n",
            "20    134\n",
            "21    154\n",
            "22    128\n",
            "23    150\n",
            "Name: count, dtype: int64\n",
            "\n",
            "계층화 샘플링 (비율: 0.3)\n",
            "샘플링 후 크기: 1376010\n",
            "자금세탁 비율: 0.2193%\n",
            "\n",
            "샘플링 후:\n",
            "  총 샘플: 1376010건\n",
            "  자금세탁: 3017건\n",
            "  자금세탁 비율: 0.2193%\n",
            "\n",
            "⚠️ 주의: 피쳐 생성에 시간이 걸릴 수 있습니다.\n",
            "최대 2000건 처리\n",
            "\n",
            "================================================================================\n",
            "집계 피쳐 생성\n",
            "================================================================================\n",
            "총 1376010건 중 2000건 처리\n",
            "진행: 0/2000 (에러: 0건)\n",
            "진행: 200/2000 (에러: 0건)\n",
            "진행: 400/2000 (에러: 0건)\n",
            "진행: 600/2000 (에러: 0건)\n",
            "진행: 800/2000 (에러: 0건)\n",
            "진행: 1000/2000 (에러: 0건)\n",
            "진행: 1200/2000 (에러: 0건)\n",
            "진행: 1400/2000 (에러: 0건)\n",
            "진행: 1600/2000 (에러: 0건)\n",
            "진행: 1800/2000 (에러: 0건)\n",
            "\n",
            "✅ 피쳐 생성 완료!\n",
            "  - 총 샘플: 2000건\n",
            "  - 생성된 피쳐 수: 66개\n",
            "  - 에러 발생: 0건\n",
            "  - 피쳐 목록: ['out_count_1h', 'out_amount_sum_1h', 'out_amount_mean_1h', 'out_amount_std_1h', 'out_amount_max_1h', 'out_amount_min_1h', 'in_count_1h', 'in_amount_sum_1h', 'in_amount_mean_1h', 'in_amount_std_1h', 'in_amount_max_1h', 'in_amount_min_1h', 'net_flow_1h', 'foreign_count_1h', 'foreign_ratio_1h', 'out_count_3h', 'out_amount_sum_3h', 'out_amount_mean_3h', 'out_amount_std_3h', 'out_amount_max_3h', 'out_amount_min_3h', 'in_count_3h', 'in_amount_sum_3h', 'in_amount_mean_3h', 'in_amount_std_3h', 'in_amount_max_3h', 'in_amount_min_3h', 'net_flow_3h', 'foreign_count_3h', 'foreign_ratio_3h', 'out_count_1d', 'out_amount_sum_1d', 'out_amount_mean_1d', 'out_amount_std_1d', 'out_amount_max_1d', 'out_amount_min_1d', 'in_count_1d', 'in_amount_sum_1d', 'in_amount_mean_1d', 'in_amount_std_1d', 'in_amount_max_1d', 'in_amount_min_1d', 'net_flow_1d', 'foreign_count_1d', 'foreign_ratio_1d', 'out_count_7d', 'out_amount_sum_7d', 'out_amount_mean_7d', 'out_amount_std_7d', 'out_amount_max_7d', 'out_amount_min_7d', 'in_count_7d', 'in_amount_sum_7d', 'in_amount_mean_7d', 'in_amount_std_7d', 'in_amount_max_7d', 'in_amount_min_7d', 'net_flow_7d', 'foreign_count_7d', 'foreign_ratio_7d', 'total_trans_count', 'total_out_amount', 'total_in_amount', 'unique_counterparties', 'unique_currencies', 'night_trans_ratio']\n",
            "\n",
            "집계 피쳐 생성 완료: 2000건\n",
            "\n",
            "================================================================================\n",
            "그래프 피쳐 생성\n",
            "================================================================================\n",
            "그래프 구축 중...\n",
            "그래프 구축 완료 - 노드: 1533021, 엣지: 1632567\n",
            "Centrality 계산 중...\n",
            "✅ 그래프 피쳐 생성 완료 - 피쳐 수: 9\n",
            "그래프 피쳐 생성 완료: 2000건\n",
            "\n",
            "그래프 피쳐 수: 9개\n",
            "그래프 피쳐: ['degree_centrality', 'in_degree_centrality', 'out_degree_centrality', 'betweenness_centrality', 'pagerank', 'num_successors', 'num_predecessors', 'total_out_weight', 'total_in_weight']\n",
            "\n",
            "피쳐 병합 후: 2000건\n",
            "전체 피쳐 수: 75\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'aml_detection_pipeline'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3363650442.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-433609930.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;31m# ========== 3.모델 학습 ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0maml_detection_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAMLModelTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAMLModelTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'aml_detection_pipeline'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 80)\n",
        "    print(\"AML 탐지 프로젝트 - 고도화 실험 시작\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\n실행 방법:\")\n",
        "    print(\"  results = main_with_experiments()\")\n",
        "    print(\"\\n포함된 실험:\")\n",
        "    print(\"  1. Baseline (그래프 피쳐 제외)\")\n",
        "    print(\"  2. 그래프 피쳐 추가\")\n",
        "    print(\"  3. 시계열 앙상블\")\n",
        "    print(\"  4. 간소화 GNN\")\n",
        "    print(\"  5. 백테스트 시뮬레이션\")\n",
        "    print(\"  6. XAI 분석 (SHAP)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 실행\n",
        "    results = main_with_experiments()"
      ],
      "metadata": {
        "id": "EPepon3npUvo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}